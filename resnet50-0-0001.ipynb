{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import sys\nimport os\n\nimport numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nfrom matplotlib import patches\n\nimport cv2\nimport torch\nimport torchvision\nfrom torchvision import datasets, models, transforms\nfrom torch import nn\nimport albumentations as A\nfrom albumentations.pytorch.transforms import ToTensorV2\nfrom torch.utils import data as torch_data\nfrom torchvision import transforms as T\nimport torch.nn.functional as F\n\nfrom torch.utils.data import Dataset\nfrom PIL import Image \nfrom xml.etree import ElementTree as ET\nimport glob \nfrom torch.utils.data import DataLoader\nfrom copy import deepcopy\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-04-19T05:53:40.427198Z","iopub.execute_input":"2023-04-19T05:53:40.428019Z","iopub.status.idle":"2023-04-19T05:53:46.246767Z","shell.execute_reply.started":"2023-04-19T05:53:40.427963Z","shell.execute_reply":"2023-04-19T05:53:46.245628Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\ndevice","metadata":{"execution":{"iopub.status.busy":"2023-04-19T05:53:54.131540Z","iopub.execute_input":"2023-04-19T05:53:54.135760Z","iopub.status.idle":"2023-04-19T05:53:54.249014Z","shell.execute_reply.started":"2023-04-19T05:53:54.135668Z","shell.execute_reply":"2023-04-19T05:53:54.247616Z"},"trusted":true},"execution_count":2,"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"device(type='cuda')"},"metadata":{}}]},{"cell_type":"markdown","source":"## 시각화","metadata":{}},{"cell_type":"code","source":"def get_image(annot):\n    img_path = '/kaggle/input/stanford-dogs-dataset/images/Images/'\n    file = annot.split('/')\n    img_filename = img_path + file[-2]+'/'+file[-1]+'.jpg'\n    return img_filename","metadata":{"execution":{"iopub.status.busy":"2023-04-17T07:13:47.680720Z","iopub.execute_input":"2023-04-17T07:13:47.681683Z","iopub.status.idle":"2023-04-17T07:13:47.688047Z","shell.execute_reply.started":"2023-04-17T07:13:47.681645Z","shell.execute_reply":"2023-04-17T07:13:47.686877Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"annotations = glob.glob('/kaggle/input/stanford-dogs-dataset/annotations/Annotation/*/*')\n\nplt.figure(figsize=(10,6))\nfor i in range(8):\n    plt.subplot(2,4,i+1)\n    plt.axis(\"off\")\n    dog = get_image(annotations[i])\n    im = Image.open(dog)\n    im = im.resize((256,256), Image.ANTIALIAS)\n    plt.imshow(im)","metadata":{"execution":{"iopub.status.busy":"2023-04-17T06:40:51.155969Z","iopub.execute_input":"2023-04-17T06:40:51.156387Z","iopub.status.idle":"2023-04-17T06:40:59.132970Z","shell.execute_reply.started":"2023-04-17T06:40:51.156350Z","shell.execute_reply":"2023-04-17T06:40:59.131621Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"root_dir = '/kaggle/input/stanford-dogs-dataset/'\nimg_dir = '/images/Images/'\nannot_dir = '/annotations/Annotation/'","metadata":{"execution":{"iopub.status.busy":"2023-04-19T05:53:59.315326Z","iopub.execute_input":"2023-04-19T05:53:59.316332Z","iopub.status.idle":"2023-04-19T05:53:59.322322Z","shell.execute_reply.started":"2023-04-19T05:53:59.316267Z","shell.execute_reply":"2023-04-19T05:53:59.321044Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"import os\n\nlabels_map = {}\nfor i, item in enumerate(os.listdir(root_dir + img_dir)):\n    sub_folder = os.path.join(root_dir + img_dir, item)\n    labels_map[sub_folder.split('-', maxsplit=3)[-1]] = i","metadata":{"execution":{"iopub.status.busy":"2023-04-19T05:54:02.952368Z","iopub.execute_input":"2023-04-19T05:54:02.953184Z","iopub.status.idle":"2023-04-19T05:54:02.980027Z","shell.execute_reply.started":"2023-04-19T05:54:02.953142Z","shell.execute_reply":"2023-04-19T05:54:02.978847Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"def img_crop(annot_path, img):\n    tree = ET.parse(annot_path)\n    obj = tree.find('./object')\n    bndbox = obj.find('bndbox')\n\n    # 강아지 종류\n    species = obj.find('name').text\n\n    # 이미지에서의 강아지 위치\n    xmin = int(bndbox.find('xmin').text)\n    ymin = int(bndbox.find('ymin').text)\n    xmax = int(bndbox.find('xmax').text)\n    ymax = int(bndbox.find('ymax').text)\n\n    cropped_img = img[ymin:ymax, xmin:xmax]\n    \n    label = labels_map.get(species)\n\n    return label, cropped_img","metadata":{"execution":{"iopub.status.busy":"2023-04-19T05:54:09.662473Z","iopub.execute_input":"2023-04-19T05:54:09.663245Z","iopub.status.idle":"2023-04-19T05:54:09.671152Z","shell.execute_reply.started":"2023-04-19T05:54:09.663208Z","shell.execute_reply":"2023-04-19T05:54:09.669853Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"class DogsDataset(Dataset):\n    def __init__(self, annot_dir, img_dir, transform=None):\n        annot_dir = glob.glob(root_dir + annot_dir + '*/*')\n        img_dir = glob.glob(root_dir + img_dir + '*/*.jpg')\n        self.annot_dir = sorted(annot_dir)\n        self.img_dir = sorted(img_dir)\n        self.transform = transform\n   \n    def __len__(self):\n        return len(self.img_dir)\n        \n    def __getitem__(self, idx):\n        annot_path = self.annot_dir[idx]\n        img_path = self.img_dir[idx]\n        \n        img = cv2.imread(img_path)\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        \n        label, img = img_crop(annot_path, img)\n        \n        if self.transform is not None:\n            img = self.transform(image=img)\n            img['label'] = label\n            return img\n            \n        sample = {'image': img, 'label': label}\n        return sample","metadata":{"execution":{"iopub.status.busy":"2023-04-19T05:54:12.710111Z","iopub.execute_input":"2023-04-19T05:54:12.710900Z","iopub.status.idle":"2023-04-19T05:54:12.720031Z","shell.execute_reply.started":"2023-04-19T05:54:12.710858Z","shell.execute_reply":"2023-04-19T05:54:12.718644Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_transform = A.Compose([A.Resize(224, 224), A.Normalize(),ToTensorV2()])","metadata":{"execution":{"iopub.status.busy":"2023-04-19T05:54:16.009259Z","iopub.execute_input":"2023-04-19T05:54:16.010897Z","iopub.status.idle":"2023-04-19T05:54:16.018808Z","shell.execute_reply.started":"2023-04-19T05:54:16.010821Z","shell.execute_reply":"2023-04-19T05:54:16.017020Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"image_dataset = DogsDataset(annot_dir ='/annotations/Annotation/',\n                            img_dir ='/images/Images/', transform=data_transform)","metadata":{"execution":{"iopub.status.busy":"2023-04-19T05:54:19.388750Z","iopub.execute_input":"2023-04-19T05:54:19.389284Z","iopub.status.idle":"2023-04-19T05:54:28.738759Z","shell.execute_reply.started":"2023-04-19T05:54:19.389238Z","shell.execute_reply":"2023-04-19T05:54:28.737624Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"len(image_dataset)","metadata":{"execution":{"iopub.status.busy":"2023-04-19T05:54:32.087708Z","iopub.execute_input":"2023-04-19T05:54:32.088512Z","iopub.status.idle":"2023-04-19T05:54:32.096929Z","shell.execute_reply.started":"2023-04-19T05:54:32.088474Z","shell.execute_reply":"2023-04-19T05:54:32.095575Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"20580"},"metadata":{}}]},{"cell_type":"code","source":"image_dataset[0]['label']","metadata":{"execution":{"iopub.status.busy":"2023-04-19T05:54:34.220612Z","iopub.execute_input":"2023-04-19T05:54:34.221314Z","iopub.status.idle":"2023-04-19T05:54:34.289954Z","shell.execute_reply.started":"2023-04-19T05:54:34.221274Z","shell.execute_reply":"2023-04-19T05:54:34.288866Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"71"},"metadata":{}}]},{"cell_type":"code","source":"image_dataset[0][\"image\"].shape","metadata":{"execution":{"iopub.status.busy":"2023-04-19T05:54:37.489607Z","iopub.execute_input":"2023-04-19T05:54:37.490105Z","iopub.status.idle":"2023-04-19T05:54:37.513029Z","shell.execute_reply.started":"2023-04-19T05:54:37.490060Z","shell.execute_reply":"2023-04-19T05:54:37.511825Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"torch.Size([3, 224, 224])"},"metadata":{}}]},{"cell_type":"code","source":"image_dataset[0][\"label\"]","metadata":{"execution":{"iopub.status.busy":"2023-04-19T05:54:39.614416Z","iopub.execute_input":"2023-04-19T05:54:39.614926Z","iopub.status.idle":"2023-04-19T05:54:39.640230Z","shell.execute_reply.started":"2023-04-19T05:54:39.614877Z","shell.execute_reply":"2023-04-19T05:54:39.639216Z"},"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"71"},"metadata":{}}]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom torch.utils.data import Subset\n\ntotal_label = [data['label'] for data in image_dataset]\n\ntrainset_idx, testset_idx = train_test_split(range(len(image_dataset)),\n                test_size=0.2, random_state=42, shuffle=True, stratify=total_label)\n\n# 전체 데이터 셋에서 train과 test로 나누기\ntrain_set = Subset(image_dataset, trainset_idx)\ntest_set = Subset(image_dataset, testset_idx)\n\n# train label\ntrain_label = [data['label'] for data in train_set]\n\n# train idx와 valid idx\ntrainset_idx, validset_idx = train_test_split(range(len(trainset_idx)),\n                test_size=0.2, random_state=42, shuffle=True, stratify=train_label)\n\n# train set에서 train과 valid로 나누기\nfrom torch.utils.data import Subset\ntrainset = Subset(train_set, trainset_idx)\nvalidset = Subset(train_set, validset_idx)","metadata":{"execution":{"iopub.status.busy":"2023-04-19T05:54:39.904135Z","iopub.execute_input":"2023-04-19T05:54:39.904600Z","iopub.status.idle":"2023-04-19T06:01:07.149921Z","shell.execute_reply.started":"2023-04-19T05:54:39.904556Z","shell.execute_reply":"2023-04-19T06:01:07.148803Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"print(type(trainset), len(trainset))\nprint(type(validset), len(validset))\nprint(type(test_set), len(test_set))","metadata":{"execution":{"iopub.status.busy":"2023-04-19T06:02:40.844410Z","iopub.execute_input":"2023-04-19T06:02:40.845506Z","iopub.status.idle":"2023-04-19T06:02:40.852594Z","shell.execute_reply.started":"2023-04-19T06:02:40.845465Z","shell.execute_reply":"2023-04-19T06:02:40.851178Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"<class 'torch.utils.data.dataset.Subset'> 13171\n<class 'torch.utils.data.dataset.Subset'> 3293\n<class 'torch.utils.data.dataset.Subset'> 4116\n","output_type":"stream"}]},{"cell_type":"code","source":"print(trainset[0]['image'].shape)\nprint(validset[0]['image'].shape)\nprint(test_set[0]['image'].shape)","metadata":{"execution":{"iopub.status.busy":"2023-04-19T06:02:43.606287Z","iopub.execute_input":"2023-04-19T06:02:43.607023Z","iopub.status.idle":"2023-04-19T06:02:43.640983Z","shell.execute_reply.started":"2023-04-19T06:02:43.606985Z","shell.execute_reply":"2023-04-19T06:02:43.639748Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"torch.Size([3, 224, 224])\ntorch.Size([3, 224, 224])\ntorch.Size([3, 224, 224])\n","output_type":"stream"}]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"batch_size = 16 # 100 -> 16\n# dataloader = DataLoader(데이터셋, 배치사이즈, 셔플여부.....)\ntrainloader = DataLoader(trainset, batch_size=batch_size, shuffle=True) # 훈련용 13171개의 데이터를 100개씩 준비\nvalidloader = DataLoader(validset, batch_size=batch_size, shuffle=False) # 검증용 10000개의 데이터를 100개씩 준비\ntestloader = DataLoader(test_set, batch_size=batch_size, shuffle=False) # 테스트용 10000개의 데이터를 100개씩 준비","metadata":{"execution":{"iopub.status.busy":"2023-04-19T06:02:47.349825Z","iopub.execute_input":"2023-04-19T06:02:47.350847Z","iopub.status.idle":"2023-04-19T06:02:47.357954Z","shell.execute_reply.started":"2023-04-19T06:02:47.350763Z","shell.execute_reply":"2023-04-19T06:02:47.356352Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"print(type(trainloader), len(trainloader))\nprint(type(validloader), len(validloader))\nprint(type(testloader), len(testloader))","metadata":{"execution":{"iopub.status.busy":"2023-04-19T06:02:49.214215Z","iopub.execute_input":"2023-04-19T06:02:49.214626Z","iopub.status.idle":"2023-04-19T06:02:49.222251Z","shell.execute_reply.started":"2023-04-19T06:02:49.214589Z","shell.execute_reply":"2023-04-19T06:02:49.220498Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"<class 'torch.utils.data.dataloader.DataLoader'> 824\n<class 'torch.utils.data.dataloader.DataLoader'> 206\n<class 'torch.utils.data.dataloader.DataLoader'> 258\n","output_type":"stream"}]},{"cell_type":"code","source":"13171/16, 3293/16, 4116/16","metadata":{"execution":{"iopub.status.busy":"2023-04-19T06:02:51.439010Z","iopub.execute_input":"2023-04-19T06:02:51.439629Z","iopub.status.idle":"2023-04-19T06:02:51.451279Z","shell.execute_reply.started":"2023-04-19T06:02:51.439591Z","shell.execute_reply":"2023-04-19T06:02:51.449883Z"},"trusted":true},"execution_count":18,"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"(823.1875, 205.8125, 257.25)"},"metadata":{}}]},{"cell_type":"code","source":"train_iter = iter(trainloader)\nbatch = next(train_iter)\nbatch['image'].size(), batch['label'].shape","metadata":{"execution":{"iopub.status.busy":"2023-04-19T06:02:54.703963Z","iopub.execute_input":"2023-04-19T06:02:54.704626Z","iopub.status.idle":"2023-04-19T06:02:54.881772Z","shell.execute_reply.started":"2023-04-19T06:02:54.704585Z","shell.execute_reply":"2023-04-19T06:02:54.880705Z"},"trusted":true},"execution_count":19,"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"(torch.Size([16, 3, 224, 224]), torch.Size([16]))"},"metadata":{}}]},{"cell_type":"code","source":"import torch.nn as nn # 파이토치에서 제공하는 다양한 계층 (Linear Layer, ....)\nimport torch.optim as optim # 옵티마이저 (경사하강법...)\nimport torch.nn.functional as F # 파이토치에서 제공하는 함수(활성화 함수...)","metadata":{"execution":{"iopub.status.busy":"2023-04-19T06:02:56.598637Z","iopub.execute_input":"2023-04-19T06:02:56.599392Z","iopub.status.idle":"2023-04-19T06:02:56.607422Z","shell.execute_reply.started":"2023-04-19T06:02:56.599352Z","shell.execute_reply":"2023-04-19T06:02:56.603662Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"import torchvision.models as models\n\nmodel = models.resnet50(weights=True)","metadata":{"execution":{"iopub.status.busy":"2023-04-19T06:02:58.251660Z","iopub.execute_input":"2023-04-19T06:02:58.252759Z","iopub.status.idle":"2023-04-19T06:02:59.781740Z","shell.execute_reply.started":"2023-04-19T06:02:58.252700Z","shell.execute_reply":"2023-04-19T06:02:59.780302Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0.00/97.8M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"52744634c22d4b518b3ad3555915a839"}},"metadata":{}}]},{"cell_type":"code","source":"for parameter in model.parameters():\n  print(parameter.requires_grad)\n# parameter들의 requires_grad 속성이 true라는 것은 오차역전파를 통해 gradient를 전달할 수 있는 상태(즉, 학습이 가능한 상태)","metadata":{"execution":{"iopub.status.busy":"2023-04-19T06:03:01.301592Z","iopub.execute_input":"2023-04-19T06:03:01.302703Z","iopub.status.idle":"2023-04-19T06:03:01.311636Z","shell.execute_reply.started":"2023-04-19T06:03:01.302664Z","shell.execute_reply":"2023-04-19T06:03:01.310377Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"True\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\n","output_type":"stream"}]},{"cell_type":"code","source":"for parameter in model.parameters():\n    parameter.requires_grad = False # 학습이 안되게 고정\n\n    for parameter in model.fc.parameters():\n        parameter.requires_grad = True # 학습이 가능한 상태 ","metadata":{"execution":{"iopub.status.busy":"2023-04-19T06:03:04.573757Z","iopub.execute_input":"2023-04-19T06:03:04.574232Z","iopub.status.idle":"2023-04-19T06:03:04.583242Z","shell.execute_reply.started":"2023-04-19T06:03:04.574194Z","shell.execute_reply":"2023-04-19T06:03:04.581900Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"# model.classifier[3] = nn.Linear(in_features=4096, out_features=512, bias=True)\n# model.classifier[6] = nn.Linear(in_features=512, out_features=2, bias=True)\nmodel.fc = nn.Linear(in_features=2048, out_features=120, bias=True)","metadata":{"execution":{"iopub.status.busy":"2023-04-19T06:03:06.441413Z","iopub.execute_input":"2023-04-19T06:03:06.442998Z","iopub.status.idle":"2023-04-19T06:03:06.452674Z","shell.execute_reply.started":"2023-04-19T06:03:06.442943Z","shell.execute_reply":"2023-04-19T06:03:06.451188Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"model.to(device)","metadata":{"execution":{"iopub.status.busy":"2023-04-19T06:03:08.153654Z","iopub.execute_input":"2023-04-19T06:03:08.154213Z","iopub.status.idle":"2023-04-19T06:03:11.340012Z","shell.execute_reply.started":"2023-04-19T06:03:08.154156Z","shell.execute_reply":"2023-04-19T06:03:11.338820Z"},"trusted":true},"execution_count":25,"outputs":[{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"ResNet(\n  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (relu): ReLU(inplace=True)\n  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n  (layer1): Sequential(\n    (0): Bottleneck(\n      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (downsample): Sequential(\n        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): Bottleneck(\n      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n    (2): Bottleneck(\n      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n  )\n  (layer2): Sequential(\n    (0): Bottleneck(\n      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (downsample): Sequential(\n        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): Bottleneck(\n      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n    (2): Bottleneck(\n      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n    (3): Bottleneck(\n      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n  )\n  (layer3): Sequential(\n    (0): Bottleneck(\n      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (downsample): Sequential(\n        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): Bottleneck(\n      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n    (2): Bottleneck(\n      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n    (3): Bottleneck(\n      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n    (4): Bottleneck(\n      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n    (5): Bottleneck(\n      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n  )\n  (layer4): Sequential(\n    (0): Bottleneck(\n      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (downsample): Sequential(\n        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): Bottleneck(\n      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n    (2): Bottleneck(\n      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n  )\n  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n  (fc): Linear(in_features=2048, out_features=120, bias=True)\n)"},"metadata":{}}]},{"cell_type":"code","source":"out = model(batch['image'].to(device))\nout.shape","metadata":{"execution":{"iopub.status.busy":"2023-04-19T06:03:15.020440Z","iopub.execute_input":"2023-04-19T06:03:15.021044Z","iopub.status.idle":"2023-04-19T06:03:19.638113Z","shell.execute_reply.started":"2023-04-19T06:03:15.021000Z","shell.execute_reply":"2023-04-19T06:03:19.636876Z"},"trusted":true},"execution_count":26,"outputs":[{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"torch.Size([16, 120])"},"metadata":{}}]},{"cell_type":"code","source":"for name, parameter in model.named_parameters():\n    print(name, parameter.size())","metadata":{"execution":{"iopub.status.busy":"2023-04-19T06:03:21.711140Z","iopub.execute_input":"2023-04-19T06:03:21.711557Z","iopub.status.idle":"2023-04-19T06:03:21.723245Z","shell.execute_reply.started":"2023-04-19T06:03:21.711520Z","shell.execute_reply":"2023-04-19T06:03:21.721773Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stdout","text":"conv1.weight torch.Size([64, 3, 7, 7])\nbn1.weight torch.Size([64])\nbn1.bias torch.Size([64])\nlayer1.0.conv1.weight torch.Size([64, 64, 1, 1])\nlayer1.0.bn1.weight torch.Size([64])\nlayer1.0.bn1.bias torch.Size([64])\nlayer1.0.conv2.weight torch.Size([64, 64, 3, 3])\nlayer1.0.bn2.weight torch.Size([64])\nlayer1.0.bn2.bias torch.Size([64])\nlayer1.0.conv3.weight torch.Size([256, 64, 1, 1])\nlayer1.0.bn3.weight torch.Size([256])\nlayer1.0.bn3.bias torch.Size([256])\nlayer1.0.downsample.0.weight torch.Size([256, 64, 1, 1])\nlayer1.0.downsample.1.weight torch.Size([256])\nlayer1.0.downsample.1.bias torch.Size([256])\nlayer1.1.conv1.weight torch.Size([64, 256, 1, 1])\nlayer1.1.bn1.weight torch.Size([64])\nlayer1.1.bn1.bias torch.Size([64])\nlayer1.1.conv2.weight torch.Size([64, 64, 3, 3])\nlayer1.1.bn2.weight torch.Size([64])\nlayer1.1.bn2.bias torch.Size([64])\nlayer1.1.conv3.weight torch.Size([256, 64, 1, 1])\nlayer1.1.bn3.weight torch.Size([256])\nlayer1.1.bn3.bias torch.Size([256])\nlayer1.2.conv1.weight torch.Size([64, 256, 1, 1])\nlayer1.2.bn1.weight torch.Size([64])\nlayer1.2.bn1.bias torch.Size([64])\nlayer1.2.conv2.weight torch.Size([64, 64, 3, 3])\nlayer1.2.bn2.weight torch.Size([64])\nlayer1.2.bn2.bias torch.Size([64])\nlayer1.2.conv3.weight torch.Size([256, 64, 1, 1])\nlayer1.2.bn3.weight torch.Size([256])\nlayer1.2.bn3.bias torch.Size([256])\nlayer2.0.conv1.weight torch.Size([128, 256, 1, 1])\nlayer2.0.bn1.weight torch.Size([128])\nlayer2.0.bn1.bias torch.Size([128])\nlayer2.0.conv2.weight torch.Size([128, 128, 3, 3])\nlayer2.0.bn2.weight torch.Size([128])\nlayer2.0.bn2.bias torch.Size([128])\nlayer2.0.conv3.weight torch.Size([512, 128, 1, 1])\nlayer2.0.bn3.weight torch.Size([512])\nlayer2.0.bn3.bias torch.Size([512])\nlayer2.0.downsample.0.weight torch.Size([512, 256, 1, 1])\nlayer2.0.downsample.1.weight torch.Size([512])\nlayer2.0.downsample.1.bias torch.Size([512])\nlayer2.1.conv1.weight torch.Size([128, 512, 1, 1])\nlayer2.1.bn1.weight torch.Size([128])\nlayer2.1.bn1.bias torch.Size([128])\nlayer2.1.conv2.weight torch.Size([128, 128, 3, 3])\nlayer2.1.bn2.weight torch.Size([128])\nlayer2.1.bn2.bias torch.Size([128])\nlayer2.1.conv3.weight torch.Size([512, 128, 1, 1])\nlayer2.1.bn3.weight torch.Size([512])\nlayer2.1.bn3.bias torch.Size([512])\nlayer2.2.conv1.weight torch.Size([128, 512, 1, 1])\nlayer2.2.bn1.weight torch.Size([128])\nlayer2.2.bn1.bias torch.Size([128])\nlayer2.2.conv2.weight torch.Size([128, 128, 3, 3])\nlayer2.2.bn2.weight torch.Size([128])\nlayer2.2.bn2.bias torch.Size([128])\nlayer2.2.conv3.weight torch.Size([512, 128, 1, 1])\nlayer2.2.bn3.weight torch.Size([512])\nlayer2.2.bn3.bias torch.Size([512])\nlayer2.3.conv1.weight torch.Size([128, 512, 1, 1])\nlayer2.3.bn1.weight torch.Size([128])\nlayer2.3.bn1.bias torch.Size([128])\nlayer2.3.conv2.weight torch.Size([128, 128, 3, 3])\nlayer2.3.bn2.weight torch.Size([128])\nlayer2.3.bn2.bias torch.Size([128])\nlayer2.3.conv3.weight torch.Size([512, 128, 1, 1])\nlayer2.3.bn3.weight torch.Size([512])\nlayer2.3.bn3.bias torch.Size([512])\nlayer3.0.conv1.weight torch.Size([256, 512, 1, 1])\nlayer3.0.bn1.weight torch.Size([256])\nlayer3.0.bn1.bias torch.Size([256])\nlayer3.0.conv2.weight torch.Size([256, 256, 3, 3])\nlayer3.0.bn2.weight torch.Size([256])\nlayer3.0.bn2.bias torch.Size([256])\nlayer3.0.conv3.weight torch.Size([1024, 256, 1, 1])\nlayer3.0.bn3.weight torch.Size([1024])\nlayer3.0.bn3.bias torch.Size([1024])\nlayer3.0.downsample.0.weight torch.Size([1024, 512, 1, 1])\nlayer3.0.downsample.1.weight torch.Size([1024])\nlayer3.0.downsample.1.bias torch.Size([1024])\nlayer3.1.conv1.weight torch.Size([256, 1024, 1, 1])\nlayer3.1.bn1.weight torch.Size([256])\nlayer3.1.bn1.bias torch.Size([256])\nlayer3.1.conv2.weight torch.Size([256, 256, 3, 3])\nlayer3.1.bn2.weight torch.Size([256])\nlayer3.1.bn2.bias torch.Size([256])\nlayer3.1.conv3.weight torch.Size([1024, 256, 1, 1])\nlayer3.1.bn3.weight torch.Size([1024])\nlayer3.1.bn3.bias torch.Size([1024])\nlayer3.2.conv1.weight torch.Size([256, 1024, 1, 1])\nlayer3.2.bn1.weight torch.Size([256])\nlayer3.2.bn1.bias torch.Size([256])\nlayer3.2.conv2.weight torch.Size([256, 256, 3, 3])\nlayer3.2.bn2.weight torch.Size([256])\nlayer3.2.bn2.bias torch.Size([256])\nlayer3.2.conv3.weight torch.Size([1024, 256, 1, 1])\nlayer3.2.bn3.weight torch.Size([1024])\nlayer3.2.bn3.bias torch.Size([1024])\nlayer3.3.conv1.weight torch.Size([256, 1024, 1, 1])\nlayer3.3.bn1.weight torch.Size([256])\nlayer3.3.bn1.bias torch.Size([256])\nlayer3.3.conv2.weight torch.Size([256, 256, 3, 3])\nlayer3.3.bn2.weight torch.Size([256])\nlayer3.3.bn2.bias torch.Size([256])\nlayer3.3.conv3.weight torch.Size([1024, 256, 1, 1])\nlayer3.3.bn3.weight torch.Size([1024])\nlayer3.3.bn3.bias torch.Size([1024])\nlayer3.4.conv1.weight torch.Size([256, 1024, 1, 1])\nlayer3.4.bn1.weight torch.Size([256])\nlayer3.4.bn1.bias torch.Size([256])\nlayer3.4.conv2.weight torch.Size([256, 256, 3, 3])\nlayer3.4.bn2.weight torch.Size([256])\nlayer3.4.bn2.bias torch.Size([256])\nlayer3.4.conv3.weight torch.Size([1024, 256, 1, 1])\nlayer3.4.bn3.weight torch.Size([1024])\nlayer3.4.bn3.bias torch.Size([1024])\nlayer3.5.conv1.weight torch.Size([256, 1024, 1, 1])\nlayer3.5.bn1.weight torch.Size([256])\nlayer3.5.bn1.bias torch.Size([256])\nlayer3.5.conv2.weight torch.Size([256, 256, 3, 3])\nlayer3.5.bn2.weight torch.Size([256])\nlayer3.5.bn2.bias torch.Size([256])\nlayer3.5.conv3.weight torch.Size([1024, 256, 1, 1])\nlayer3.5.bn3.weight torch.Size([1024])\nlayer3.5.bn3.bias torch.Size([1024])\nlayer4.0.conv1.weight torch.Size([512, 1024, 1, 1])\nlayer4.0.bn1.weight torch.Size([512])\nlayer4.0.bn1.bias torch.Size([512])\nlayer4.0.conv2.weight torch.Size([512, 512, 3, 3])\nlayer4.0.bn2.weight torch.Size([512])\nlayer4.0.bn2.bias torch.Size([512])\nlayer4.0.conv3.weight torch.Size([2048, 512, 1, 1])\nlayer4.0.bn3.weight torch.Size([2048])\nlayer4.0.bn3.bias torch.Size([2048])\nlayer4.0.downsample.0.weight torch.Size([2048, 1024, 1, 1])\nlayer4.0.downsample.1.weight torch.Size([2048])\nlayer4.0.downsample.1.bias torch.Size([2048])\nlayer4.1.conv1.weight torch.Size([512, 2048, 1, 1])\nlayer4.1.bn1.weight torch.Size([512])\nlayer4.1.bn1.bias torch.Size([512])\nlayer4.1.conv2.weight torch.Size([512, 512, 3, 3])\nlayer4.1.bn2.weight torch.Size([512])\nlayer4.1.bn2.bias torch.Size([512])\nlayer4.1.conv3.weight torch.Size([2048, 512, 1, 1])\nlayer4.1.bn3.weight torch.Size([2048])\nlayer4.1.bn3.bias torch.Size([2048])\nlayer4.2.conv1.weight torch.Size([512, 2048, 1, 1])\nlayer4.2.bn1.weight torch.Size([512])\nlayer4.2.bn1.bias torch.Size([512])\nlayer4.2.conv2.weight torch.Size([512, 512, 3, 3])\nlayer4.2.bn2.weight torch.Size([512])\nlayer4.2.bn2.bias torch.Size([512])\nlayer4.2.conv3.weight torch.Size([2048, 512, 1, 1])\nlayer4.2.bn3.weight torch.Size([2048])\nlayer4.2.bn3.bias torch.Size([2048])\nfc.weight torch.Size([120, 2048])\nfc.bias torch.Size([120])\n","output_type":"stream"}]},{"cell_type":"code","source":"learning_rate = 0.0001\n# 손실함수\nloss_fn = nn.CrossEntropyLoss()\n\n# 옵티마이저(최적화함수, 예:경사하강법)\n# optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n\n# 규제의 강도 설정 weight_decay\noptimizer = optim.Adam(model.parameters(), lr=learning_rate)\n# optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=0.001)\n\n# Learning Rate Schedule\n# https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.ReduceLROnPlateau.html\n\n# 모니터링하고 있는 값(예:valid_loss)의 최소값(min) 또는 최대값(max) patience 기간동안 줄어들지 않을 때(OnPlateau) lr에 factor(0.1)를 곱해주는 전략\nscheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=4, verbose=True)","metadata":{"execution":{"iopub.status.busy":"2023-04-19T06:03:34.717823Z","iopub.execute_input":"2023-04-19T06:03:34.718593Z","iopub.status.idle":"2023-04-19T06:03:34.728533Z","shell.execute_reply.started":"2023-04-19T06:03:34.718551Z","shell.execute_reply":"2023-04-19T06:03:34.725220Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"def validate(model, validloader, loss_fn):\n    total = 0   \n    correct = 0\n    valid_loss = 0\n    valid_accuracy = 0\n\n  # 전방향 예측을 구할 때는 gradient가 필요가 없음음\n    with torch.no_grad():\n        for batch in validloader:# 이터레이터로부터 next()가 호출되며 미니배치를 반환(images, labels)      \n          # images, labels : (torch.Size([16, 3, 224, 224]), torch.Size([16]))\n          # 0. Data를 GPU로 보내기\n            images = batch['image']\n            labels = batch['label']\n            \n            images, labels = images.to(device), labels.to(device)\n            \n            # 1. 입력 데이터 준비\n            # not Flatten !!\n            # images.resize_(images.size()[0], 784)\n  \n            # 2. 전방향(Forward) 예측\n            logit = model(images) # 예측 점수\n            _, preds = torch.max(logit, 1) # 배치에 대한 최종 예측\n            # preds = logit.max(dim=1)[1] \n            correct += int((preds == labels).sum()) # 배치 중 맞은 것의 개수가 correct에 누적\n            total += labels.shape[0] # 배치 사이즈만큼씩 total에 누적\n\n            loss = loss_fn(logit, labels)\n            valid_loss += loss.item() # tensor에서 값을 꺼내와서, 배치의 loss 평균값을 valid_loss에 누적\n\n        valid_accuracy = correct / total\n  \n    return valid_loss, valid_accuracy","metadata":{"execution":{"iopub.status.busy":"2023-04-19T06:03:38.988367Z","iopub.execute_input":"2023-04-19T06:03:38.988830Z","iopub.status.idle":"2023-04-19T06:03:38.999652Z","shell.execute_reply.started":"2023-04-19T06:03:38.988757Z","shell.execute_reply":"2023-04-19T06:03:38.998260Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"def train_loop(model, trainloader, loss_fn, epochs, optimizer):  \n    steps = 0\n    steps_per_epoch = len(trainloader) \n    min_loss = 1000000\n    max_accuracy = 0\n    trigger = 0\n    patience = 7 \n\n    for epoch in range(epochs):\n        model.train() # 훈련 모드\n        train_loss = 0\n        for batch in trainloader: # 이터레이터로부터 next()가 호출되며 미니배치를 반환(images, labels)\n            steps += 1\n            # images, labels : (torch.Size([16, 3, 224, 224]), torch.Size([16]))\n            # 0. Data를 GPU로 보내기\n            images = batch['image']\n            labels = batch['label']\n            images, labels = images.to(device), labels.to(device)\n\n            # 1. 입력 데이터 준비\n            # not Flatten !!\n            # images.resize_(images.shape[0], 784) \n\n            # 2. 전방향(forward) 예측\n            predict = model(images) # 예측 점수\n            loss = loss_fn(predict, labels) # 예측 점수와 정답을 CrossEntropyLoss에 넣어 Loss값 반환\n\n            # 3. 역방향(backward) 오차(Gradient) 전파\n            optimizer.zero_grad() # Gradient가 누적되지 않게 하기 위해\n            loss.backward() # 모델파리미터들의 Gradient 전파\n\n            # 4. 경사 하강법으로 모델 파라미터 업데이트\n            optimizer.step() # W <- W -lr*Gradient\n\n            train_loss += loss.item()\n            if (steps % steps_per_epoch) == 0 : \n                model.eval() # 평가 모드 : 평가에서 사용하지 않을 계층(배치 정규화, 드롭아웃)들을 수행하지 않게 하기 위해서\n                valid_loss, valid_accuracy = validate(model, validloader, loss_fn)\n            # -------------------------------------------\n\n                print('Epoch : {}/{}.......'.format(epoch+1, epochs),            \n                   'Train Loss : {:.3f}'.format(train_loss/len(trainloader)), \n                   'Valid Loss : {:.3f}'.format(valid_loss/len(validloader)), \n                   'Valid Accuracy : {:.3f}'.format(valid_accuracy)            \n                      )\n\n              # Best model 저장    \n              # option 1 : valid_loss 모니터링\n              # if valid_loss < min_loss: # 바로 이전 epoch의 loss보다 작으면 저장하기\n              #   min_loss = valid_loss\n              #   best_model_state = deepcopy(model.state_dict())          \n              #   torch.save(best_model_state, 'best_checkpoint.pth')     \n\n              # option 2 : valid_accuracy 모니터링      \n                if valid_accuracy > max_accuracy : # 바로 이전 epoch의 accuracy보다 크면 저장하기\n                    max_accuracy = valid_accuracy\n                    best_model_state = deepcopy(model.state_dict())          \n                    torch.save(best_model_state, 'best_checkpoint.pth')  \n              # -------------------------------------------\n\n              # Early Stopping (조기 종료)\n                if valid_loss > min_loss: # valid_loss가 min_loss를 갱신하지 못하면\n                    trigger += 1\n                    print('trigger : ', trigger)\n                if trigger > patience:\n                    print('Early Stopping !!!')\n                    print('Training loop is finished !!')\n                    return\n                else:\n                    trigger = 0\n                    min_loss = valid_loss\n                # -------------------------------------------\n\n                # Learning Rate Scheduler\n                scheduler.step(valid_loss)\n            # -------------------------------------------\n\n    return  ","metadata":{"execution":{"iopub.status.busy":"2023-04-19T06:03:40.925782Z","iopub.execute_input":"2023-04-19T06:03:40.926406Z","iopub.status.idle":"2023-04-19T06:03:40.940142Z","shell.execute_reply.started":"2023-04-19T06:03:40.926365Z","shell.execute_reply":"2023-04-19T06:03:40.938697Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"epochs = 55\n%time train_loop(model, trainloader, loss_fn, epochs, optimizer)","metadata":{"execution":{"iopub.status.busy":"2023-04-19T06:03:42.838571Z","iopub.execute_input":"2023-04-19T06:03:42.839556Z","iopub.status.idle":"2023-04-19T08:04:53.642301Z","shell.execute_reply.started":"2023-04-19T06:03:42.839500Z","shell.execute_reply":"2023-04-19T08:04:53.641051Z"},"trusted":true},"execution_count":31,"outputs":[{"name":"stdout","text":"Epoch : 1/55....... Train Loss : 3.256 Valid Loss : 1.840 Valid Accuracy : 0.745\nEpoch : 2/55....... Train Loss : 1.622 Valid Loss : 1.061 Valid Accuracy : 0.809\nEpoch : 3/55....... Train Loss : 1.121 Valid Loss : 0.810 Valid Accuracy : 0.813\nEpoch : 4/55....... Train Loss : 0.904 Valid Loss : 0.714 Valid Accuracy : 0.817\nEpoch : 5/55....... Train Loss : 0.788 Valid Loss : 0.615 Valid Accuracy : 0.841\nEpoch : 6/55....... Train Loss : 0.710 Valid Loss : 0.587 Valid Accuracy : 0.840\nEpoch : 7/55....... Train Loss : 0.651 Valid Loss : 0.577 Valid Accuracy : 0.826\nEpoch : 8/55....... Train Loss : 0.607 Valid Loss : 0.570 Valid Accuracy : 0.833\nEpoch : 9/55....... Train Loss : 0.580 Valid Loss : 0.542 Valid Accuracy : 0.833\nEpoch : 10/55....... Train Loss : 0.555 Valid Loss : 0.530 Valid Accuracy : 0.839\nEpoch : 11/55....... Train Loss : 0.526 Valid Loss : 0.540 Valid Accuracy : 0.833\ntrigger :  1\nEpoch : 12/55....... Train Loss : 0.506 Valid Loss : 0.505 Valid Accuracy : 0.844\nEpoch : 13/55....... Train Loss : 0.481 Valid Loss : 0.520 Valid Accuracy : 0.840\ntrigger :  1\nEpoch : 14/55....... Train Loss : 0.465 Valid Loss : 0.504 Valid Accuracy : 0.841\nEpoch : 15/55....... Train Loss : 0.450 Valid Loss : 0.504 Valid Accuracy : 0.843\ntrigger :  1\nEpoch : 16/55....... Train Loss : 0.441 Valid Loss : 0.496 Valid Accuracy : 0.845\nEpoch : 17/55....... Train Loss : 0.424 Valid Loss : 0.502 Valid Accuracy : 0.843\ntrigger :  1\nEpoch : 18/55....... Train Loss : 0.409 Valid Loss : 0.504 Valid Accuracy : 0.840\ntrigger :  1\nEpoch : 19/55....... Train Loss : 0.414 Valid Loss : 0.518 Valid Accuracy : 0.833\ntrigger :  1\nEpoch : 20/55....... Train Loss : 0.400 Valid Loss : 0.512 Valid Accuracy : 0.840\nEpoch : 21/55....... Train Loss : 0.384 Valid Loss : 0.480 Valid Accuracy : 0.846\nEpoch : 22/55....... Train Loss : 0.381 Valid Loss : 0.518 Valid Accuracy : 0.838\ntrigger :  1\nEpoch : 23/55....... Train Loss : 0.368 Valid Loss : 0.518 Valid Accuracy : 0.838\nEpoch : 24/55....... Train Loss : 0.364 Valid Loss : 0.508 Valid Accuracy : 0.842\nEpoch : 25/55....... Train Loss : 0.359 Valid Loss : 0.489 Valid Accuracy : 0.848\nEpoch : 26/55....... Train Loss : 0.353 Valid Loss : 0.508 Valid Accuracy : 0.845\ntrigger :  1\nEpoch 00026: reducing learning rate of group 0 to 1.0000e-05.\nEpoch : 27/55....... Train Loss : 0.328 Valid Loss : 0.479 Valid Accuracy : 0.845\nEpoch : 28/55....... Train Loss : 0.325 Valid Loss : 0.503 Valid Accuracy : 0.842\ntrigger :  1\nEpoch : 29/55....... Train Loss : 0.320 Valid Loss : 0.501 Valid Accuracy : 0.843\nEpoch : 30/55....... Train Loss : 0.323 Valid Loss : 0.492 Valid Accuracy : 0.845\nEpoch : 31/55....... Train Loss : 0.313 Valid Loss : 0.504 Valid Accuracy : 0.840\ntrigger :  1\nEpoch : 32/55....... Train Loss : 0.319 Valid Loss : 0.499 Valid Accuracy : 0.842\nEpoch 00032: reducing learning rate of group 0 to 1.0000e-06.\nEpoch : 33/55....... Train Loss : 0.317 Valid Loss : 0.500 Valid Accuracy : 0.844\ntrigger :  1\nEpoch : 34/55....... Train Loss : 0.311 Valid Loss : 0.470 Valid Accuracy : 0.848\nEpoch : 35/55....... Train Loss : 0.317 Valid Loss : 0.482 Valid Accuracy : 0.850\ntrigger :  1\nEpoch : 36/55....... Train Loss : 0.313 Valid Loss : 0.488 Valid Accuracy : 0.842\ntrigger :  1\nEpoch : 37/55....... Train Loss : 0.314 Valid Loss : 0.490 Valid Accuracy : 0.845\ntrigger :  1\nEpoch : 38/55....... Train Loss : 0.310 Valid Loss : 0.490 Valid Accuracy : 0.849\ntrigger :  1\nEpoch : 39/55....... Train Loss : 0.309 Valid Loss : 0.473 Valid Accuracy : 0.850\nEpoch 00039: reducing learning rate of group 0 to 1.0000e-07.\nEpoch : 40/55....... Train Loss : 0.314 Valid Loss : 0.502 Valid Accuracy : 0.845\ntrigger :  1\nEpoch : 41/55....... Train Loss : 0.314 Valid Loss : 0.484 Valid Accuracy : 0.849\nEpoch : 42/55....... Train Loss : 0.308 Valid Loss : 0.483 Valid Accuracy : 0.848\nEpoch : 43/55....... Train Loss : 0.309 Valid Loss : 0.485 Valid Accuracy : 0.849\ntrigger :  1\nEpoch : 44/55....... Train Loss : 0.316 Valid Loss : 0.488 Valid Accuracy : 0.844\ntrigger :  1\nEpoch 00044: reducing learning rate of group 0 to 1.0000e-08.\nEpoch : 45/55....... Train Loss : 0.319 Valid Loss : 0.476 Valid Accuracy : 0.846\nEpoch : 46/55....... Train Loss : 0.308 Valid Loss : 0.500 Valid Accuracy : 0.846\ntrigger :  1\nEpoch : 47/55....... Train Loss : 0.313 Valid Loss : 0.477 Valid Accuracy : 0.852\nEpoch : 48/55....... Train Loss : 0.312 Valid Loss : 0.468 Valid Accuracy : 0.850\nEpoch : 49/55....... Train Loss : 0.309 Valid Loss : 0.485 Valid Accuracy : 0.850\ntrigger :  1\nEpoch : 50/55....... Train Loss : 0.320 Valid Loss : 0.474 Valid Accuracy : 0.849\nEpoch : 51/55....... Train Loss : 0.316 Valid Loss : 0.489 Valid Accuracy : 0.848\ntrigger :  1\nEpoch : 52/55....... Train Loss : 0.313 Valid Loss : 0.471 Valid Accuracy : 0.853\nEpoch : 53/55....... Train Loss : 0.313 Valid Loss : 0.491 Valid Accuracy : 0.845\ntrigger :  1\nEpoch : 54/55....... Train Loss : 0.311 Valid Loss : 0.473 Valid Accuracy : 0.849\nEpoch : 55/55....... Train Loss : 0.311 Valid Loss : 0.486 Valid Accuracy : 0.844\ntrigger :  1\nCPU times: user 1h 56min, sys: 15min 9s, total: 2h 11min 9s\nWall time: 2h 1min 10s\n","output_type":"stream"}]},{"cell_type":"code","source":"test_iter = iter(testloader)\nbatch = next(test_iter)\nimages = batch['image']\nlabels = batch['label']\n\nimages, labels = images.to(device), labels.to(device)\nprint(images.size(), labels.size())","metadata":{"execution":{"iopub.status.busy":"2023-04-19T08:05:17.245679Z","iopub.execute_input":"2023-04-19T08:05:17.246429Z","iopub.status.idle":"2023-04-19T08:05:17.383104Z","shell.execute_reply.started":"2023-04-19T08:05:17.246388Z","shell.execute_reply":"2023-04-19T08:05:17.381878Z"},"trusted":true},"execution_count":32,"outputs":[{"name":"stdout","text":"torch.Size([16, 3, 224, 224]) torch.Size([16])\n","output_type":"stream"}]},{"cell_type":"code","source":"def evaluation(model, testloader, loss_fn):\n    total = 0   \n    correct = 0\n    test_loss = 0\n    test_accuracy = 0\n\n  # 전방향 예측을 구할 때는 gradient가 필요가 없음음\n    with torch.no_grad():\n        for images, labels in testloader: # 이터레이터로부터 next()가 호출되며 미니배치를 반환(images, labels)\n          # 0. Data를 GPU로 보내기\n            images = batch['image']\n            labels = batch['label']\n            images, labels = images.to(device), labels.to(device)\n          # 1. 입력 데이터 준비\n          # not Flatten\n          # images.resize_(images.size()[0], 784)\n      \n          # 2. 전방향(Forward) 예측\n            logit = model(images) # 예측 점수\n            _, preds = torch.max(logit, 1) # 배치에 대한 최종 예측\n            # preds = logit.max(dim=1)[1] \n            correct += int((preds == labels).sum()) # 배치치 중 맞은 것의 개수가 correct에 누적\n            total += labels.shape[0] # 배치 사이즈만큼씩 total에 누적\n\n            loss = loss_fn(logit, labels)\n            test_loss += loss.item() # tensor에서 값을 꺼내와서, 배치의 loss 평균값을 valid_loss에 누적\n        \n        test_accuracy = correct / total\n   \n        print('Test Loss : {:.3f}'.format(test_loss/len(testloader)), \n        'Test Accuracy : {:.3f}'.format(test_accuracy))\n\nmodel.eval()\nevaluation(model, testloader, loss_fn)  ","metadata":{"execution":{"iopub.status.busy":"2023-04-19T08:06:40.363857Z","iopub.execute_input":"2023-04-19T08:06:40.365027Z","iopub.status.idle":"2023-04-19T08:07:14.200713Z","shell.execute_reply.started":"2023-04-19T08:06:40.364983Z","shell.execute_reply":"2023-04-19T08:07:14.199577Z"},"trusted":true},"execution_count":35,"outputs":[{"name":"stdout","text":"Test Loss : 0.286 Test Accuracy : 0.875\n","output_type":"stream"}]},{"cell_type":"code","source":"torch.save(model.state_dict(), 'last_checkpoint_5.pth')","metadata":{"execution":{"iopub.status.busy":"2023-04-19T08:07:14.202916Z","iopub.execute_input":"2023-04-19T08:07:14.203620Z","iopub.status.idle":"2023-04-19T08:07:14.438874Z","shell.execute_reply.started":"2023-04-19T08:07:14.203575Z","shell.execute_reply":"2023-04-19T08:07:14.437778Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"# 시간이 흐른뒤 다시 모델 가져오기\nlast_state_dict_5 = torch.load('last_checkpoint_5.pth')","metadata":{"execution":{"iopub.status.busy":"2023-04-19T08:07:21.378312Z","iopub.execute_input":"2023-04-19T08:07:21.378731Z","iopub.status.idle":"2023-04-19T08:07:21.472199Z","shell.execute_reply.started":"2023-04-19T08:07:21.378694Z","shell.execute_reply":"2023-04-19T08:07:21.470852Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"# 읽어들인 모델 파라미터는 모델 아키텍처에 연결을 시켜줘야 함\n# load_state_dict() 사용\nlast_model_5 = model\nlast_model_5.to(device)\nlast_model_5.load_state_dict(last_state_dict_5)","metadata":{"execution":{"iopub.status.busy":"2023-04-19T08:07:23.640118Z","iopub.execute_input":"2023-04-19T08:07:23.640744Z","iopub.status.idle":"2023-04-19T08:07:23.663688Z","shell.execute_reply.started":"2023-04-19T08:07:23.640704Z","shell.execute_reply":"2023-04-19T08:07:23.662468Z"},"trusted":true},"execution_count":38,"outputs":[{"execution_count":38,"output_type":"execute_result","data":{"text/plain":"<All keys matched successfully>"},"metadata":{}}]},{"cell_type":"code","source":"last_model_5.eval()\nevaluation(last_model_5, testloader, loss_fn)  ","metadata":{"execution":{"iopub.status.busy":"2023-04-19T08:07:26.188673Z","iopub.execute_input":"2023-04-19T08:07:26.189413Z","iopub.status.idle":"2023-04-19T08:07:58.268940Z","shell.execute_reply.started":"2023-04-19T08:07:26.189373Z","shell.execute_reply":"2023-04-19T08:07:58.267837Z"},"trusted":true},"execution_count":39,"outputs":[{"name":"stdout","text":"Test Loss : 0.286 Test Accuracy : 0.875\n","output_type":"stream"}]},{"cell_type":"code","source":"# valid loss or accuracy 기준 best model\nbest_state_dict_5 = torch.load('best_checkpoint.pth')\nbest_model_5 = model\nbest_model_5.to(device)\nbest_model_5.load_state_dict(best_state_dict_5)","metadata":{"execution":{"iopub.status.busy":"2023-04-19T08:08:12.928932Z","iopub.execute_input":"2023-04-19T08:08:12.929305Z","iopub.status.idle":"2023-04-19T08:08:13.040077Z","shell.execute_reply.started":"2023-04-19T08:08:12.929273Z","shell.execute_reply":"2023-04-19T08:08:13.038864Z"},"trusted":true},"execution_count":40,"outputs":[{"execution_count":40,"output_type":"execute_result","data":{"text/plain":"<All keys matched successfully>"},"metadata":{}}]},{"cell_type":"code","source":"best_model_5.eval()\nevaluation(best_model_5, testloader, loss_fn)","metadata":{"execution":{"iopub.status.busy":"2023-04-19T08:08:15.440574Z","iopub.execute_input":"2023-04-19T08:08:15.441241Z","iopub.status.idle":"2023-04-19T08:08:48.289605Z","shell.execute_reply.started":"2023-04-19T08:08:15.441200Z","shell.execute_reply":"2023-04-19T08:08:48.288477Z"},"trusted":true},"execution_count":41,"outputs":[{"name":"stdout","text":"Test Loss : 0.227 Test Accuracy : 0.938\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}