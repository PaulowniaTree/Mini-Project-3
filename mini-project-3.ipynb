{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import sys\nimport os\n\nimport numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nfrom matplotlib import patches\n\nimport cv2\nimport torch\nimport torchvision\nfrom torchvision import datasets, models, transforms\nfrom torch import nn\nimport albumentations as A\nfrom albumentations.pytorch.transforms import ToTensorV2\nfrom torch.utils import data as torch_data\nfrom torchvision import transforms as T\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor\nfrom torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\nfrom torchvision.models.detection import FasterRCNN\n\nfrom torch.utils.data import Dataset\nfrom PIL import Image \nfrom xml.etree import ElementTree as ET\nimport glob \nfrom torch.utils.data import DataLoader\nfrom copy import deepcopy","metadata":{"execution":{"iopub.status.busy":"2023-04-17T02:56:34.503132Z","iopub.execute_input":"2023-04-17T02:56:34.504138Z","iopub.status.idle":"2023-04-17T02:56:34.512460Z","shell.execute_reply.started":"2023-04-17T02:56:34.504099Z","shell.execute_reply":"2023-04-17T02:56:34.511390Z"},"trusted":true},"execution_count":59,"outputs":[]},{"cell_type":"code","source":"%pwd","metadata":{"execution":{"iopub.status.busy":"2023-04-17T01:56:36.248933Z","iopub.execute_input":"2023-04-17T01:56:36.249367Z","iopub.status.idle":"2023-04-17T01:56:36.258706Z","shell.execute_reply.started":"2023-04-17T01:56:36.249322Z","shell.execute_reply":"2023-04-17T01:56:36.257633Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"'/kaggle/working'"},"metadata":{}}]},{"cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\ndevice","metadata":{"execution":{"iopub.status.busy":"2023-04-17T02:33:15.071196Z","iopub.execute_input":"2023-04-17T02:33:15.071630Z","iopub.status.idle":"2023-04-17T02:33:15.156928Z","shell.execute_reply.started":"2023-04-17T02:33:15.071589Z","shell.execute_reply":"2023-04-17T02:33:15.155671Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"device(type='cuda')"},"metadata":{}}]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"images = '/kaggle/input/stanford-dogs-dataset/images/'\nannotations = '/kaggle/input/stanford-dogs-dataset/annotations'","metadata":{"execution":{"iopub.status.busy":"2023-04-17T01:56:39.503253Z","iopub.execute_input":"2023-04-17T01:56:39.503977Z","iopub.status.idle":"2023-04-17T01:56:39.508679Z","shell.execute_reply.started":"2023-04-17T01:56:39.503937Z","shell.execute_reply":"2023-04-17T01:56:39.507538Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"def get_image(annot):\n    img_path = '/kaggle/input/stanford-dogs-dataset/images/Images/'\n    file = annot.split('/')\n    img_filename = img_path + file[-2]+'/'+file[-1]+'.jpg'\n    return img_filename","metadata":{"execution":{"iopub.status.busy":"2023-04-15T14:10:25.139793Z","iopub.status.idle":"2023-04-15T14:10:25.140537Z","shell.execute_reply.started":"2023-04-15T14:10:25.140277Z","shell.execute_reply":"2023-04-15T14:10:25.140304Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"annotations = glob.glob('/kaggle/input/stanford-dogs-dataset/annotations/Annotation/*/*')\n\nplt.figure(figsize=(10,6))\nfor i in range(8):\n    plt.subplot(2,4,i+1)\n    plt.axis(\"off\")\n    dog = get_image(annotations[i])\n    im = Image.open(dog)\n    im = im.resize((256,256), Image.ANTIALIAS)\n    plt.imshow(im)","metadata":{"execution":{"iopub.status.busy":"2023-04-15T14:10:25.141873Z","iopub.status.idle":"2023-04-15T14:10:25.142614Z","shell.execute_reply.started":"2023-04-15T14:10:25.142349Z","shell.execute_reply":"2023-04-15T14:10:25.142375Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"root_dir = '/kaggle/input/stanford-dogs-dataset/'\nimg_dir = '/images/Images/'\nannot_dir = '/annotations/Annotation/'","metadata":{"execution":{"iopub.status.busy":"2023-04-17T02:33:20.500505Z","iopub.execute_input":"2023-04-17T02:33:20.501218Z","iopub.status.idle":"2023-04-17T02:33:20.505883Z","shell.execute_reply.started":"2023-04-17T02:33:20.501180Z","shell.execute_reply":"2023-04-17T02:33:20.504782Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"import os\n\nlabels_map = {}\nfor i, item in enumerate(os.listdir(root_dir + img_dir)):\n    sub_folder = os.path.join(root_dir + img_dir, item)\n    labels_map[sub_folder.split('-', maxsplit=3)[-1]] = i","metadata":{"execution":{"iopub.status.busy":"2023-04-17T02:33:24.652773Z","iopub.execute_input":"2023-04-17T02:33:24.653366Z","iopub.status.idle":"2023-04-17T02:33:24.670556Z","shell.execute_reply.started":"2023-04-17T02:33:24.653328Z","shell.execute_reply":"2023-04-17T02:33:24.669566Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"labels_map","metadata":{"execution":{"iopub.status.busy":"2023-04-17T02:33:26.244500Z","iopub.execute_input":"2023-04-17T02:33:26.244874Z","iopub.status.idle":"2023-04-17T02:33:26.257255Z","shell.execute_reply.started":"2023-04-17T02:33:26.244842Z","shell.execute_reply":"2023-04-17T02:33:26.255951Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"{'otterhound': 0,\n 'cocker_spaniel': 1,\n 'Brittany_spaniel': 2,\n 'Afghan_hound': 3,\n 'Maltese_dog': 4,\n 'schipperke': 5,\n 'Irish_setter': 6,\n 'Pekinese': 7,\n 'golden_retriever': 8,\n 'vizsla': 9,\n 'Welsh_springer_spaniel': 10,\n 'Staffordshire_bullterrier': 11,\n 'Border_collie': 12,\n 'Irish_terrier': 13,\n 'Eskimo_dog': 14,\n 'pug': 15,\n 'kelpie': 16,\n 'Yorkshire_terrier': 17,\n 'Tibetan_terrier': 18,\n 'Walker_hound': 19,\n 'affenpinscher': 20,\n 'Cardigan': 21,\n 'English_springer': 22,\n 'English_foxhound': 23,\n 'West_Highland_white_terrier': 24,\n 'Lakeland_terrier': 25,\n 'Rhodesian_ridgeback': 26,\n 'Gordon_setter': 27,\n 'Lhasa': 28,\n 'curly-coated_retriever': 29,\n 'beagle': 30,\n 'Tibetan_mastiff': 31,\n 'Sussex_spaniel': 32,\n 'Saint_Bernard': 33,\n 'toy_terrier': 34,\n 'standard_poodle': 35,\n 'Bernese_mountain_dog': 36,\n 'Pomeranian': 37,\n 'Ibizan_hound': 38,\n 'redbone': 39,\n 'toy_poodle': 40,\n 'basset': 41,\n 'Scottish_deerhound': 42,\n 'miniature_pinscher': 43,\n 'basenji': 44,\n 'Border_terrier': 45,\n 'Bedlington_terrier': 46,\n 'Kerry_blue_terrier': 47,\n 'Weimaraner': 48,\n 'English_setter': 49,\n 'bluetick': 50,\n 'Boston_bull': 51,\n 'Italian_greyhound': 52,\n 'Dandie_Dinmont': 53,\n 'Airedale': 54,\n 'Irish_water_spaniel': 55,\n 'Norfolk_terrier': 56,\n 'wire-haired_fox_terrier': 57,\n 'French_bulldog': 58,\n 'soft-coated_wheaten_terrier': 59,\n 'komondor': 60,\n 'African_hunting_dog': 61,\n 'Siberian_husky': 62,\n 'Newfoundland': 63,\n 'Bouvier_des_Flandres': 64,\n 'Saluki': 65,\n 'Shetland_sheepdog': 66,\n 'collie': 67,\n 'Rottweiler': 68,\n 'silky_terrier': 69,\n 'Norwegian_elkhound': 70,\n 'Chihuahua': 71,\n 'Leonberg': 72,\n 'Norwich_terrier': 73,\n 'cairn': 74,\n 'boxer': 75,\n 'borzoi': 76,\n 'dhole': 77,\n 'Samoyed': 78,\n 'German_shepherd': 79,\n 'Labrador_retriever': 80,\n 'Blenheim_spaniel': 81,\n 'groenendael': 82,\n 'Doberman': 83,\n 'Great_Dane': 84,\n 'flat-coated_retriever': 85,\n 'Appenzeller': 86,\n 'Shih-Tzu': 87,\n 'Japanese_spaniel': 88,\n 'Greater_Swiss_Mountain_dog': 89,\n 'black-and-tan_coonhound': 90,\n 'dingo': 91,\n 'Great_Pyrenees': 92,\n 'whippet': 93,\n 'keeshond': 94,\n 'malinois': 95,\n 'American_Staffordshire_terrier': 96,\n 'Mexican_hairless': 97,\n 'giant_schnauzer': 98,\n 'Brabancon_griffon': 99,\n 'kuvasz': 100,\n 'miniature_poodle': 101,\n 'Irish_wolfhound': 102,\n 'briard': 103,\n 'clumber': 104,\n 'standard_schnauzer': 105,\n 'bull_mastiff': 106,\n 'malamute': 107,\n 'Sealyham_terrier': 108,\n 'EntleBucher': 109,\n 'chow': 110,\n 'papillon': 111,\n 'Pembroke': 112,\n 'German_short-haired_pointer': 113,\n 'Old_English_sheepdog': 114,\n 'Chesapeake_Bay_retriever': 115,\n 'Scotch_terrier': 116,\n 'Australian_terrier': 117,\n 'miniature_schnauzer': 118,\n 'bloodhound': 119}"},"metadata":{}}]},{"cell_type":"code","source":"def img_crop(annot_path, img):\n    tree = ET.parse(annot_path)\n    obj = tree.find('./object')\n    bndbox = obj.find('bndbox')\n\n    # 강아지 종류\n    species = obj.find('name').text\n\n    # 이미지에서의 강아지 위치\n    xmin = int(bndbox.find('xmin').text)\n    ymin = int(bndbox.find('ymin').text)\n    xmax = int(bndbox.find('xmax').text)\n    ymax = int(bndbox.find('ymax').text)\n\n    cropped_img = img[ymin:ymax, xmin:xmax]\n    \n    label = labels_map.get(species)\n\n    return label, cropped_img","metadata":{"execution":{"iopub.status.busy":"2023-04-17T02:33:30.437644Z","iopub.execute_input":"2023-04-17T02:33:30.438346Z","iopub.status.idle":"2023-04-17T02:33:30.445475Z","shell.execute_reply.started":"2023-04-17T02:33:30.438309Z","shell.execute_reply":"2023-04-17T02:33:30.443766Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"class DogsDataset(Dataset):\n    def __init__(self, annot_dir, img_dir, transform=None):\n        annot_dir = glob.glob(root_dir + annot_dir + '*/*')\n        img_dir = glob.glob(root_dir + img_dir + '*/*.jpg')\n        self.annot_dir = sorted(annot_dir)\n        self.img_dir = sorted(img_dir)\n        self.transform = transform\n   \n    def __len__(self):\n        return len(self.img_dir)\n        \n    def __getitem__(self, idx):\n        annot_path = self.annot_dir[idx]\n        img_path = self.img_dir[idx]\n        \n        img = cv2.imread(img_path)\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        \n        label, img = img_crop(annot_path, img)\n        \n        if self.transform is not None:\n            img = self.transform(image=img)\n            img['label'] = label\n            return img\n            \n        sample = {'image': img, 'label': label}\n        return sample","metadata":{"execution":{"iopub.status.busy":"2023-04-17T02:33:33.281524Z","iopub.execute_input":"2023-04-17T02:33:33.281893Z","iopub.status.idle":"2023-04-17T02:33:33.291291Z","shell.execute_reply.started":"2023-04-17T02:33:33.281861Z","shell.execute_reply":"2023-04-17T02:33:33.290054Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"data_transform = A.Compose([A.Resize(224, 224), A.Normalize(),ToTensorV2()])","metadata":{"execution":{"iopub.status.busy":"2023-04-17T02:33:36.312644Z","iopub.execute_input":"2023-04-17T02:33:36.313027Z","iopub.status.idle":"2023-04-17T02:33:36.319385Z","shell.execute_reply.started":"2023-04-17T02:33:36.312995Z","shell.execute_reply":"2023-04-17T02:33:36.317751Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"image_dataset = DogsDataset(annot_dir ='/annotations/Annotation/',\n                            img_dir ='/images/Images/', transform=data_transform)","metadata":{"execution":{"iopub.status.busy":"2023-04-17T02:33:38.857184Z","iopub.execute_input":"2023-04-17T02:33:38.857538Z","iopub.status.idle":"2023-04-17T02:33:43.662060Z","shell.execute_reply.started":"2023-04-17T02:33:38.857507Z","shell.execute_reply":"2023-04-17T02:33:43.661007Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"len(image_dataset)","metadata":{"execution":{"iopub.status.busy":"2023-04-17T02:34:22.313773Z","iopub.execute_input":"2023-04-17T02:34:22.314442Z","iopub.status.idle":"2023-04-17T02:34:22.320585Z","shell.execute_reply.started":"2023-04-17T02:34:22.314404Z","shell.execute_reply":"2023-04-17T02:34:22.319595Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"20580"},"metadata":{}}]},{"cell_type":"code","source":"image_dataset[0]","metadata":{"execution":{"iopub.status.busy":"2023-04-17T02:34:25.326290Z","iopub.execute_input":"2023-04-17T02:34:25.327184Z","iopub.status.idle":"2023-04-17T02:34:25.470775Z","shell.execute_reply.started":"2023-04-17T02:34:25.327131Z","shell.execute_reply":"2023-04-17T02:34:25.469692Z"},"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"{'image': tensor([[[-1.0048, -1.0048, -1.0390,  ...,  0.4508,  0.5022,  0.2453],\n          [-1.0219, -1.0390, -1.0733,  ..., -0.1314, -0.3027, -0.4739],\n          [-1.0390, -1.0562, -1.0904,  ..., -0.1314, -0.3883, -0.6965],\n          ...,\n          [ 2.2489,  2.2489,  2.2489,  ..., -2.0323, -2.0152, -1.9124],\n          [ 2.2489,  2.2489,  2.2489,  ..., -2.0152, -2.0494, -1.9638],\n          [ 2.2489,  2.2489,  2.2489,  ..., -2.0494, -2.0837, -2.0837]],\n \n         [[-1.1604, -1.1604, -1.1954,  ...,  0.8529,  0.9405,  0.6954],\n          [-1.1779, -1.1954, -1.2304,  ...,  0.2052,  0.0301, -0.1099],\n          [-1.1954, -1.2129, -1.2479,  ...,  0.1702, -0.0749, -0.3550],\n          ...,\n          [ 2.4286,  2.4286,  2.4286,  ..., -1.9482, -1.9307, -1.8256],\n          [ 2.4286,  2.4286,  2.4286,  ..., -1.9307, -1.9657, -1.8782],\n          [ 2.4286,  2.4286,  2.4286,  ..., -1.8957, -1.9307, -1.9307]],\n \n         [[-0.9853, -1.0027, -1.0201,  ...,  0.3045,  0.3742,  0.1128],\n          [-1.0027, -1.0201, -1.0550,  ..., -0.5844, -0.7587, -0.8807],\n          [-1.0201, -1.0376, -1.0724,  ..., -0.7413, -1.0201, -1.3339],\n          ...,\n          [ 2.6400,  2.6400,  2.6400,  ..., -1.7173, -1.6999, -1.5953],\n          [ 2.6400,  2.6400,  2.6400,  ..., -1.6999, -1.7347, -1.6476],\n          [ 2.6400,  2.6400,  2.6400,  ..., -1.6824, -1.7173, -1.7173]]]),\n 'label': 71}"},"metadata":{}}]},{"cell_type":"code","source":"image_dataset[0]['label']","metadata":{"execution":{"iopub.status.busy":"2023-04-17T02:34:27.563347Z","iopub.execute_input":"2023-04-17T02:34:27.564027Z","iopub.status.idle":"2023-04-17T02:34:27.578534Z","shell.execute_reply.started":"2023-04-17T02:34:27.563990Z","shell.execute_reply":"2023-04-17T02:34:27.577400Z"},"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"71"},"metadata":{}}]},{"cell_type":"code","source":"image_dataset[0]['image'].shape","metadata":{"execution":{"iopub.status.busy":"2023-04-17T02:34:29.596315Z","iopub.execute_input":"2023-04-17T02:34:29.596793Z","iopub.status.idle":"2023-04-17T02:34:29.619082Z","shell.execute_reply.started":"2023-04-17T02:34:29.596746Z","shell.execute_reply":"2023-04-17T02:34:29.618105Z"},"trusted":true},"execution_count":14,"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"torch.Size([3, 224, 224])"},"metadata":{}}]},{"cell_type":"code","source":"d = image_dataset[0]\nd['label']","metadata":{"execution":{"iopub.status.busy":"2023-04-17T02:34:31.209882Z","iopub.execute_input":"2023-04-17T02:34:31.210346Z","iopub.status.idle":"2023-04-17T02:34:31.228005Z","shell.execute_reply.started":"2023-04-17T02:34:31.210305Z","shell.execute_reply":"2023-04-17T02:34:31.226669Z"},"trusted":true},"execution_count":15,"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"71"},"metadata":{}}]},{"cell_type":"code","source":"len(image_dataset)","metadata":{"execution":{"iopub.status.busy":"2023-04-17T02:34:33.254029Z","iopub.execute_input":"2023-04-17T02:34:33.254411Z","iopub.status.idle":"2023-04-17T02:34:33.261075Z","shell.execute_reply.started":"2023-04-17T02:34:33.254380Z","shell.execute_reply":"2023-04-17T02:34:33.260037Z"},"trusted":true},"execution_count":16,"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"20580"},"metadata":{}}]},{"cell_type":"code","source":"#total_label = [data['label'] for data in image_dataset]","metadata":{"execution":{"iopub.status.busy":"2023-04-17T01:57:23.723940Z","iopub.execute_input":"2023-04-17T01:57:23.724855Z","iopub.status.idle":"2023-04-17T02:00:55.294870Z","shell.execute_reply.started":"2023-04-17T01:57:23.724802Z","shell.execute_reply":"2023-04-17T02:00:55.293769Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"# total_label =[]\n# for i in range(len(image_dataset)):\n#     total_label.append(image_dataset[i]['label'])","metadata":{"execution":{"iopub.status.busy":"2023-04-17T01:42:30.501939Z","iopub.execute_input":"2023-04-17T01:42:30.502728Z","iopub.status.idle":"2023-04-17T01:42:43.417421Z","shell.execute_reply.started":"2023-04-17T01:42:30.502680Z","shell.execute_reply":"2023-04-17T01:42:43.415990Z"},"trusted":true},"execution_count":14,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_23/4283122383.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtotal_label\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mtotal_label\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_dataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/tmp/ipykernel_23/3209242307.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mimg_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimg_dir\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOLOR_BGR2RGB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"# total_label=[]\n# for data in image_dataset:\n#     total_label.append(data['label'])","metadata":{"execution":{"iopub.status.busy":"2023-04-15T14:10:25.173453Z","iopub.status.idle":"2023-04-15T14:10:25.174192Z","shell.execute_reply.started":"2023-04-15T14:10:25.173935Z","shell.execute_reply":"2023-04-15T14:10:25.173961Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"****데이터셋 나누기****","metadata":{}},{"cell_type":"code","source":"# train과  test 나누기 (인덱싱)\nfrom sklearn.model_selection import train_test_split\n\ntrain_indices, test_indices = train_test_split(\n                            range(len(image_dataset)), # X의 index\n                            test_size= 0.2, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2023-04-17T02:01:36.501187Z","iopub.execute_input":"2023-04-17T02:01:36.501712Z","iopub.status.idle":"2023-04-17T02:01:36.521788Z","shell.execute_reply.started":"2023-04-17T02:01:36.501656Z","shell.execute_reply":"2023-04-17T02:01:36.520798Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"len(train_indices), len(test_indices)","metadata":{"execution":{"iopub.status.busy":"2023-04-17T02:01:38.596160Z","iopub.execute_input":"2023-04-17T02:01:38.596553Z","iopub.status.idle":"2023-04-17T02:01:38.604508Z","shell.execute_reply.started":"2023-04-17T02:01:38.596515Z","shell.execute_reply":"2023-04-17T02:01:38.603002Z"},"trusted":true},"execution_count":20,"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"(16464, 4116)"},"metadata":{}}]},{"cell_type":"code","source":"# train set과  test set 나누기 \nfrom torch.utils.data import Subset\ntrain_set = Subset(image_dataset, train_indices)\ntest_set = Subset(image_dataset, test_indices)","metadata":{"execution":{"iopub.status.busy":"2023-04-17T02:01:41.110100Z","iopub.execute_input":"2023-04-17T02:01:41.110464Z","iopub.status.idle":"2023-04-17T02:01:41.115867Z","shell.execute_reply.started":"2023-04-17T02:01:41.110432Z","shell.execute_reply":"2023-04-17T02:01:41.114831Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"print(type(train_set), len(train_set))\nprint(type(test_set), len(test_set))","metadata":{"execution":{"iopub.status.busy":"2023-04-17T02:01:43.145453Z","iopub.execute_input":"2023-04-17T02:01:43.145854Z","iopub.status.idle":"2023-04-17T02:01:43.154336Z","shell.execute_reply.started":"2023-04-17T02:01:43.145819Z","shell.execute_reply":"2023-04-17T02:01:43.153167Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"<class 'torch.utils.data.dataset.Subset'> 16464\n<class 'torch.utils.data.dataset.Subset'> 4116\n","output_type":"stream"}]},{"cell_type":"code","source":"train_set[0]['image']","metadata":{"execution":{"iopub.status.busy":"2023-04-17T02:01:47.544891Z","iopub.execute_input":"2023-04-17T02:01:47.545282Z","iopub.status.idle":"2023-04-17T02:01:47.566993Z","shell.execute_reply.started":"2023-04-17T02:01:47.545248Z","shell.execute_reply":"2023-04-17T02:01:47.565774Z"},"trusted":true},"execution_count":23,"outputs":[{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"tensor([[[-1.2959, -0.1143, -0.0287,  ..., -1.6727, -1.6555, -1.7754],\n         [-1.2274, -0.5767, -0.2513,  ..., -1.5699, -1.3987, -1.7754],\n         [-1.3987, -0.6794, -0.4397,  ..., -1.6727, -1.7412, -1.7583],\n         ...,\n         [ 0.0741,  0.1426,  0.0227,  ...,  0.0398,  0.0912,  0.2796],\n         [ 0.1083, -0.0116,  0.0227,  ...,  0.9132,  0.3994,  0.6049],\n         [ 0.0398,  0.0227,  0.0741,  ...,  0.5878,  0.6221,  0.5878]],\n\n        [[-1.9482, -0.7927, -0.6877,  ..., -1.4405, -1.4055, -1.4930],\n         [-1.7381, -1.1078, -0.7752,  ..., -1.3529, -1.1604, -1.4755],\n         [-1.7206, -1.0203, -0.8102,  ..., -1.4755, -1.5105, -1.4755],\n         ...,\n         [-0.4601, -0.4076, -0.5826,  ..., -0.3200, -0.2850, -0.0399],\n         [-0.4951, -0.6527, -0.6527,  ...,  0.6429,  0.1176,  0.3803],\n         [-0.5826, -0.6176, -0.6001,  ...,  0.3803,  0.4328,  0.4328]],\n\n        [[-1.7696, -0.7413, -0.6890,  ..., -0.8807, -0.8458, -0.9504],\n         [-1.6824, -1.0898, -0.8110,  ..., -0.7413, -0.6018, -0.9678],\n         [-1.7347, -1.0376, -0.8807,  ..., -0.8633, -0.9504, -0.9504],\n         ...,\n         [-0.4275, -0.3753, -0.5147,  ..., -0.1487, -0.1138,  0.1128],\n         [-0.4798, -0.6193, -0.5844,  ...,  0.8274,  0.3045,  0.5485],\n         [-0.5147, -0.5495, -0.5147,  ...,  0.5834,  0.6356,  0.6182]]])"},"metadata":{}}]},{"cell_type":"code","source":"# train set에서 train과 valid로 나누기(인덱싱)\nfrom sklearn.model_selection import train_test_split\n\ntrain_set_indices, valid_indices = train_test_split(\n                            range(len(train_set)), # X의 index                        \n                            test_size= 0.2, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2023-04-17T02:01:49.818745Z","iopub.execute_input":"2023-04-17T02:01:49.819374Z","iopub.status.idle":"2023-04-17T02:01:49.830054Z","shell.execute_reply.started":"2023-04-17T02:01:49.819326Z","shell.execute_reply":"2023-04-17T02:01:49.829055Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"len(train_set_indices), len(valid_indices)","metadata":{"execution":{"iopub.status.busy":"2023-04-17T02:01:51.487753Z","iopub.execute_input":"2023-04-17T02:01:51.488693Z","iopub.status.idle":"2023-04-17T02:01:51.496077Z","shell.execute_reply.started":"2023-04-17T02:01:51.488641Z","shell.execute_reply":"2023-04-17T02:01:51.494721Z"},"trusted":true},"execution_count":25,"outputs":[{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"(13171, 3293)"},"metadata":{}}]},{"cell_type":"code","source":"(13171+ 3293)","metadata":{"execution":{"iopub.status.busy":"2023-04-17T02:01:53.644325Z","iopub.execute_input":"2023-04-17T02:01:53.645295Z","iopub.status.idle":"2023-04-17T02:01:53.652480Z","shell.execute_reply.started":"2023-04-17T02:01:53.645254Z","shell.execute_reply":"2023-04-17T02:01:53.651247Z"},"trusted":true},"execution_count":26,"outputs":[{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"16464"},"metadata":{}}]},{"cell_type":"code","source":"# train set에서 train과 valid로 나누기\nfrom torch.utils.data import Subset\ntrainset = Subset(train_set, train_set_indices)\nvalidset = Subset(train_set, valid_indices)","metadata":{"execution":{"iopub.status.busy":"2023-04-17T02:01:55.812413Z","iopub.execute_input":"2023-04-17T02:01:55.813606Z","iopub.status.idle":"2023-04-17T02:01:55.819878Z","shell.execute_reply.started":"2023-04-17T02:01:55.813553Z","shell.execute_reply":"2023-04-17T02:01:55.818763Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom torch.utils.data import Subset\n\ntotal_label = [data['label'] for data in image_dataset]\n\ntrainset_idx, testset_idx = train_test_split(range(len(image_dataset)),\n                test_size=0.2, random_state=42, shuffle=True, stratify=total_label)\n\n# 전체 데이터 셋이서 train과 test로 나누기\ntrain_set = Subset(image_dataset, trainset_idx)\ntest_set = Subset(image_dataset, testset_idx)\n\n# train label\ntrain_label = [data['label'] for data in train_set]\n\n# train idx와 valid idx\ntrainset_idx, validset_idx = train_test_split(range(len(trainset_idx)),\n                test_size=0.2, random_state=42, shuffle=True, stratify=train_label)\n\n# train set에서 train과 valid로 나누기\nfrom torch.utils.data import Subset\ntrainset = Subset(train_set, trainset_idx)\nvalidset = Subset(train_set, validset_idx)","metadata":{"execution":{"iopub.status.busy":"2023-04-17T02:34:58.552448Z","iopub.execute_input":"2023-04-17T02:34:58.553167Z","iopub.status.idle":"2023-04-17T02:40:25.369342Z","shell.execute_reply.started":"2023-04-17T02:34:58.553117Z","shell.execute_reply":"2023-04-17T02:40:25.368296Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"print(type(trainset), len(trainset))\nprint(type(validset), len(validset))\nprint(type(test_set), len(test_set))","metadata":{"execution":{"iopub.status.busy":"2023-04-17T02:42:19.081340Z","iopub.execute_input":"2023-04-17T02:42:19.081944Z","iopub.status.idle":"2023-04-17T02:42:19.088287Z","shell.execute_reply.started":"2023-04-17T02:42:19.081905Z","shell.execute_reply":"2023-04-17T02:42:19.087150Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"<class 'torch.utils.data.dataset.Subset'> 13171\n<class 'torch.utils.data.dataset.Subset'> 3293\n<class 'torch.utils.data.dataset.Subset'> 4116\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2023-04-17T01:54:32.312102Z","iopub.execute_input":"2023-04-17T01:54:32.312795Z","iopub.status.idle":"2023-04-17T01:54:32.319564Z","shell.execute_reply.started":"2023-04-17T01:54:32.312757Z","shell.execute_reply":"2023-04-17T01:54:32.318458Z"},"trusted":true},"execution_count":21,"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"29635"},"metadata":{}}]},{"cell_type":"code","source":"trainset[0]['image'].shape","metadata":{"execution":{"iopub.status.busy":"2023-04-17T02:42:23.018227Z","iopub.execute_input":"2023-04-17T02:42:23.018788Z","iopub.status.idle":"2023-04-17T02:42:23.051637Z","shell.execute_reply.started":"2023-04-17T02:42:23.018737Z","shell.execute_reply":"2023-04-17T02:42:23.050319Z"},"trusted":true},"execution_count":19,"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"torch.Size([3, 224, 224])"},"metadata":{}}]},{"cell_type":"code","source":"validset[0]['image'].shape","metadata":{"execution":{"iopub.status.busy":"2023-04-17T02:42:23.310694Z","iopub.execute_input":"2023-04-17T02:42:23.311174Z","iopub.status.idle":"2023-04-17T02:42:23.328855Z","shell.execute_reply.started":"2023-04-17T02:42:23.311137Z","shell.execute_reply":"2023-04-17T02:42:23.327788Z"},"trusted":true},"execution_count":20,"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"torch.Size([3, 224, 224])"},"metadata":{}}]},{"cell_type":"code","source":"test_set[0]['image'].shape","metadata":{"execution":{"iopub.status.busy":"2023-04-17T02:42:23.674538Z","iopub.execute_input":"2023-04-17T02:42:23.674950Z","iopub.status.idle":"2023-04-17T02:42:23.685941Z","shell.execute_reply.started":"2023-04-17T02:42:23.674912Z","shell.execute_reply":"2023-04-17T02:42:23.684779Z"},"trusted":true},"execution_count":21,"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"torch.Size([3, 224, 224])"},"metadata":{}}]},{"cell_type":"markdown","source":"****ResNet50 모델**** ","metadata":{}},{"cell_type":"code","source":"batch_size = 16 # 100 -> 16\n# dataloader = DataLoader(데이터셋, 배치사이즈, 셔플여부.....)\ntrainloader = DataLoader(trainset, batch_size=batch_size, shuffle=True) # 훈련용 13171개의 데이터를 100개씩 준비\nvalidloader = DataLoader(validset, batch_size=batch_size, shuffle=False) # 검증용 10000개의 데이터를 100개씩 준비\ntestloader = DataLoader(test_set, batch_size=batch_size, shuffle=False) # 테스트용 10000개의 데이터를 100개씩 준비","metadata":{"execution":{"iopub.status.busy":"2023-04-17T02:42:25.899176Z","iopub.execute_input":"2023-04-17T02:42:25.899542Z","iopub.status.idle":"2023-04-17T02:42:25.905916Z","shell.execute_reply.started":"2023-04-17T02:42:25.899507Z","shell.execute_reply":"2023-04-17T02:42:25.904788Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"print(type(trainloader), len(trainloader))\nprint(type(validloader), len(validloader))\nprint(type(testloader), len(testloader))","metadata":{"execution":{"iopub.status.busy":"2023-04-17T02:42:26.186435Z","iopub.execute_input":"2023-04-17T02:42:26.186812Z","iopub.status.idle":"2023-04-17T02:42:26.193074Z","shell.execute_reply.started":"2023-04-17T02:42:26.186779Z","shell.execute_reply":"2023-04-17T02:42:26.191926Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"<class 'torch.utils.data.dataloader.DataLoader'> 824\n<class 'torch.utils.data.dataloader.DataLoader'> 206\n<class 'torch.utils.data.dataloader.DataLoader'> 258\n","output_type":"stream"}]},{"cell_type":"code","source":"13171/16, 3293/16, 4116/16","metadata":{"execution":{"iopub.status.busy":"2023-04-17T02:42:28.225934Z","iopub.execute_input":"2023-04-17T02:42:28.226307Z","iopub.status.idle":"2023-04-17T02:42:28.233380Z","shell.execute_reply.started":"2023-04-17T02:42:28.226274Z","shell.execute_reply":"2023-04-17T02:42:28.232420Z"},"trusted":true},"execution_count":24,"outputs":[{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"(823.1875, 205.8125, 257.25)"},"metadata":{}}]},{"cell_type":"code","source":"train_iter = iter(trainloader)\nbatch = next(train_iter)\nbatch['image'].size(), batch['label'].shape","metadata":{"execution":{"iopub.status.busy":"2023-04-17T02:42:30.155233Z","iopub.execute_input":"2023-04-17T02:42:30.155596Z","iopub.status.idle":"2023-04-17T02:42:30.279857Z","shell.execute_reply.started":"2023-04-17T02:42:30.155562Z","shell.execute_reply":"2023-04-17T02:42:30.278792Z"},"trusted":true},"execution_count":25,"outputs":[{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"(torch.Size([16, 3, 224, 224]), torch.Size([16]))"},"metadata":{}}]},{"cell_type":"code","source":"import torch.nn as nn # 파이토치에서 제공하는 다양한 계층 (Linear Layer, ....)\nimport torch.optim as optim # 옵티마이저 (경사하강법...)\nimport torch.nn.functional as F # 파이토치에서 제공하는 함수(활성화 함수...)","metadata":{"execution":{"iopub.status.busy":"2023-04-17T02:42:32.269840Z","iopub.execute_input":"2023-04-17T02:42:32.270465Z","iopub.status.idle":"2023-04-17T02:42:32.276135Z","shell.execute_reply.started":"2023-04-17T02:42:32.270431Z","shell.execute_reply":"2023-04-17T02:42:32.275001Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"markdown","source":"**** conv block 별 사이즈 확인****","metadata":{}},{"cell_type":"code","source":"# conv1\nconv1 = nn.Sequential(\n    # BatchNorm 계층은 편향값의 효과를 보완해주므로 관례상 생략\n                            nn.Conv2d(in_channels=3, out_channels=64, kernel_size=7, stride=2, padding=3, bias=False), # [16, 65, 112, 112]\n                            nn.BatchNorm2d(num_features=64),\n                            nn.ReLU(),                        \n                            nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n                            ) # [16, 64, 56, 56]\nconv1_out = conv1(batch['image'])                                     \nconv1_out.shape           ","metadata":{"execution":{"iopub.status.busy":"2023-04-17T02:02:38.582683Z","iopub.execute_input":"2023-04-17T02:02:38.583127Z","iopub.status.idle":"2023-04-17T02:02:39.042575Z","shell.execute_reply.started":"2023-04-17T02:02:38.583086Z","shell.execute_reply":"2023-04-17T02:02:39.041448Z"},"trusted":true},"execution_count":37,"outputs":[{"execution_count":37,"output_type":"execute_result","data":{"text/plain":"torch.Size([16, 64, 56, 56])"},"metadata":{}}]},{"cell_type":"code","source":"# conv2_x      \nshortcut2 = nn.Sequential(\n                                nn.Conv2d(in_channels=64, out_channels=256, kernel_size=1, stride=1), \n                                nn.BatchNorm2d(num_features=256)                                  \n                              )  \nconv2_x = nn.Sequential(\n                              ResBlock(in_channels=64, out_channels=64, shortcut=shortcut2, stride=1),                                 \n                              ResBlock(in_channels=256, out_channels=64, shortcut=None, stride=1),\n                              ResBlock(in_channels=256, out_channels=64, shortcut=None, stride=1)\n                            ) # [16, 256, 56, 56]\nconv2_x_out = conv2_x(conv1_out)                                     \nconv2_x_out.shape          ","metadata":{"execution":{"iopub.status.busy":"2023-04-17T02:02:58.799985Z","iopub.execute_input":"2023-04-17T02:02:58.800356Z","iopub.status.idle":"2023-04-17T02:02:59.987900Z","shell.execute_reply.started":"2023-04-17T02:02:58.800322Z","shell.execute_reply":"2023-04-17T02:02:59.986902Z"},"trusted":true},"execution_count":41,"outputs":[{"execution_count":41,"output_type":"execute_result","data":{"text/plain":"torch.Size([16, 256, 56, 56])"},"metadata":{}}]},{"cell_type":"code","source":"# conv3_x\nshortcut3 = nn.Sequential(\n                                nn.Conv2d(in_channels=256, out_channels=512, kernel_size=1, stride=2), \n                                nn.BatchNorm2d(num_features=512)                                  \n                              )      \nconv3_x = nn.Sequential(\n                              ResBlock(in_channels=256, out_channels=128, shortcut=shortcut3, stride=2),\n                              ResBlock(in_channels=512, out_channels=128, shortcut=None, stride=1),\n                              ResBlock(in_channels=512, out_channels=128, shortcut=None, stride=1),\n                              ResBlock(in_channels=512, out_channels=128, shortcut=None, stride=1)\n\n                            ) # [16, 512, 28, 28] \nconv3_x_out = conv3_x(conv2_x_out)                                     \nconv3_x_out.shape       ","metadata":{"execution":{"iopub.status.busy":"2023-04-17T02:03:03.002513Z","iopub.execute_input":"2023-04-17T02:03:03.003213Z","iopub.status.idle":"2023-04-17T02:03:03.914237Z","shell.execute_reply.started":"2023-04-17T02:03:03.003176Z","shell.execute_reply":"2023-04-17T02:03:03.913069Z"},"trusted":true},"execution_count":42,"outputs":[{"execution_count":42,"output_type":"execute_result","data":{"text/plain":"torch.Size([16, 512, 28, 28])"},"metadata":{}}]},{"cell_type":"code","source":"# conv4_x\nshortcut4 = nn.Sequential(\n                                nn.Conv2d(in_channels=512, out_channels=1024, kernel_size=1, stride=2), \n                                nn.BatchNorm2d(num_features=1024)                                  \n                              )      \nconv4_x = nn.Sequential(\n                              ResBlock(in_channels=512, out_channels=256, shortcut=shortcut4, stride=2),\n                              ResBlock(in_channels=1024, out_channels=256, shortcut=None, stride=1),\n                              ResBlock(in_channels=1024, out_channels=256, shortcut=None, stride=1),\n                              ResBlock(in_channels=1024, out_channels=256, shortcut=None, stride=1),\n                              ResBlock(in_channels=1024, out_channels=256, shortcut=None, stride=1),\n                              ResBlock(in_channels=1024, out_channels=256, shortcut=None, stride=1),                                                                                        \n                            ) # [16, 1024, 14, 14]\nconv4_x_out = conv4_x(conv3_x_out)                                     \nconv4_x_out.shape ","metadata":{"execution":{"iopub.status.busy":"2023-04-17T02:03:05.879716Z","iopub.execute_input":"2023-04-17T02:03:05.880660Z","iopub.status.idle":"2023-04-17T02:03:06.653556Z","shell.execute_reply.started":"2023-04-17T02:03:05.880605Z","shell.execute_reply":"2023-04-17T02:03:06.652587Z"},"trusted":true},"execution_count":43,"outputs":[{"execution_count":43,"output_type":"execute_result","data":{"text/plain":"torch.Size([16, 1024, 14, 14])"},"metadata":{}}]},{"cell_type":"code","source":"# conv5_x\nshortcut5 = nn.Sequential(\n                                nn.Conv2d(in_channels=1024, out_channels=2048, kernel_size=1, stride=2), \n                                nn.BatchNorm2d(num_features=2048)                                  \n                              )    \nconv5_x = nn.Sequential(\n                              ResBlock(in_channels=1024, out_channels=512, shortcut=shortcut5, stride=2),\n                              ResBlock(in_channels=2048, out_channels=512, shortcut=None, stride=1),\n                              ResBlock(in_channels=2048, out_channels=512, shortcut=None, stride=1),                                                                \n                            ) # [16, 2048, 7, 7]  \nconv5_x_out = conv5_x(conv4_x_out)                                     \nconv5_x_out.shape  ","metadata":{"execution":{"iopub.status.busy":"2023-04-17T02:03:08.721174Z","iopub.execute_input":"2023-04-17T02:03:08.721559Z","iopub.status.idle":"2023-04-17T02:03:09.821604Z","shell.execute_reply.started":"2023-04-17T02:03:08.721521Z","shell.execute_reply":"2023-04-17T02:03:09.820557Z"},"trusted":true},"execution_count":44,"outputs":[{"execution_count":44,"output_type":"execute_result","data":{"text/plain":"torch.Size([16, 2048, 7, 7])"},"metadata":{}}]},{"cell_type":"code","source":"avg_pool = nn.AdaptiveAvgPool2d((1, 1))  # [16, 2048, 1, 1]  \navg_pool_out = avg_pool(conv5_x_out)\navg_pool_out.shape","metadata":{"execution":{"iopub.status.busy":"2023-04-17T02:03:11.513380Z","iopub.execute_input":"2023-04-17T02:03:11.514090Z","iopub.status.idle":"2023-04-17T02:03:11.525330Z","shell.execute_reply.started":"2023-04-17T02:03:11.514053Z","shell.execute_reply":"2023-04-17T02:03:11.523769Z"},"trusted":true},"execution_count":45,"outputs":[{"execution_count":45,"output_type":"execute_result","data":{"text/plain":"torch.Size([16, 2048, 1, 1])"},"metadata":{}}]},{"cell_type":"code","source":"class ResBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, shortcut=None, stride=1): # shortcut에 계층을 설정되어 있다면 그 계층을 통과한뒤 Add\n        super().__init__()\n\n        # 1x1 conv\n        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0)\n        self.batch_norm1 = nn.BatchNorm2d(out_channels)\n\n        # 3x3 conv \n        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=stride, padding=1) # stride=2일 경우에는 downsampling\n        self.batch_norm2 = nn.BatchNorm2d(out_channels)\n\n        # 1x1 conv\n        self.conv3 = nn.Conv2d(out_channels, out_channels*4, kernel_size=1, stride=1, padding=0)\n        self.batch_norm3 = nn.BatchNorm2d(out_channels*4)\n\n        self.shortcut = shortcut\n        self.stride = stride\n        self.relu = nn.ReLU()\n\n    def forward(self, x):\n        identity = x.clone()\n        x = self.relu(self.batch_norm1(self.conv1(x)))\n        x = self.relu(self.batch_norm2(self.conv2(x)))\n        x = self.batch_norm3(self.conv3(x))\n\n        # shortcut 계층을 바깥에서 설정한것을 적용할 때\n        if self.shortcut is not None:\n            identity = self.shortcut(identity)\n\n        x += identity  # x = x+identity\n        x = self.relu(x)\n\n        return x\n","metadata":{"execution":{"iopub.status.busy":"2023-04-17T02:57:00.313231Z","iopub.execute_input":"2023-04-17T02:57:00.313836Z","iopub.status.idle":"2023-04-17T02:57:00.330036Z","shell.execute_reply.started":"2023-04-17T02:57:00.313796Z","shell.execute_reply":"2023-04-17T02:57:00.328941Z"},"trusted":true},"execution_count":60,"outputs":[]},{"cell_type":"code","source":"class ResNet50(nn.Module):\n    def __init__(self):\n        super().__init__()\n        # conv1\n        self.conv1 = nn.Sequential(\n                                    # BatchNorm 계층은 편향값의 효과를 보완해주므로 관례상 생략략\n                                    nn.Conv2d(in_channels=3, out_channels=64, kernel_size=7, stride=2, padding=3, bias=False),\n                                    nn.BatchNorm2d(num_features=64),\n                                    nn.ReLU(),                        \n                                    nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n                                    ) # [16, 64, 56, 56]\n        # conv2_x      \n        self.shortcut2 = nn.Sequential(\n                                        nn.Conv2d(in_channels=64, out_channels=256, kernel_size=1, stride=1), \n                                        nn.BatchNorm2d(num_features=256)                                  \n                                      )              \n        self.conv2_x = nn.Sequential(\n                                      ResBlock(in_channels=64, out_channels=64, shortcut=self.shortcut2, stride=1),                                 \n                                      ResBlock(in_channels=256, out_channels=64, shortcut=None, stride=1),\n                                      ResBlock(in_channels=256, out_channels=64, shortcut=None, stride=1)\n                                    ) # [16, 256, 56, 56]\n        # conv3_x\n        self.shortcut3 = nn.Sequential(\n                                        nn.Conv2d(in_channels=256, out_channels=512, kernel_size=1, stride=2), \n                                        nn.BatchNorm2d(num_features=512)                                  \n                                      )      \n        self.conv3_x = nn.Sequential(\n                                      ResBlock(in_channels=256, out_channels=128, shortcut=self.shortcut3, stride=2),\n                                      ResBlock(in_channels=512, out_channels=128, shortcut=None, stride=1),\n                                      ResBlock(in_channels=512, out_channels=128, shortcut=None, stride=1),\n                                      ResBlock(in_channels=512, out_channels=128, shortcut=None, stride=1)\n\n                                    ) # [16, 512, 28, 28]   \n        # conv4_x\n        self.shortcut4 = nn.Sequential(\n                                        nn.Conv2d(in_channels=512, out_channels=1024, kernel_size=1, stride=2), \n                                        nn.BatchNorm2d(num_features=1024)                                  \n                                      )      \n        self.conv4_x = nn.Sequential(\n                                     ResBlock(in_channels=512, out_channels=256, shortcut=self.shortcut4, stride=2),\n                                     ResBlock(in_channels=1024, out_channels=256, shortcut=None, stride=1),\n                                     ResBlock(in_channels=1024, out_channels=256, shortcut=None, stride=1),\n                                     ResBlock(in_channels=1024, out_channels=256, shortcut=None, stride=1),\n                                     ResBlock(in_channels=1024, out_channels=256, shortcut=None, stride=1),\n                                     ResBlock(in_channels=1024, out_channels=256, shortcut=None, stride=1),                                                                                        \n                                    ) # [16, 1024, 14, 14] \n        # conv5_x\n        self.shortcut5 = nn.Sequential(\n                                        nn.Conv2d(in_channels=1024, out_channels=2048, kernel_size=1, stride=2), \n                                        nn.BatchNorm2d(num_features=2048)                                  \n                                      )    \n        self.conv5_x = nn.Sequential(\n                                     ResBlock(in_channels=1024, out_channels=512, shortcut=self.shortcut5, stride=2),\n                                     ResBlock(in_channels=2048, out_channels=512, shortcut=None, stride=1),\n                                     ResBlock(in_channels=2048, out_channels=512, shortcut=None, stride=1),                                                                \n                                    ) # [16, 2048, 7, 7]  \n\n        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))  # [16, 2048, 1, 1]                                                                                                                              \n\n        self.classifier = nn.Sequential(\n                                    nn.Linear(in_features=2048, out_features=120),\n                                    # nn.BatchNorm1d(num_features=64),\n                                    # nn.ReLU(),\n                                    # nn.Linear(in_features=64, out_features=2)\n                                    )\n\n    def forward(self, x):\n        x = self.conv1(x) # [16, 64, 56, 56]\n        x = self.conv2_x(x) # [16, 256, 56, 56]\n        x = self.conv3_x(x) # [16, 512, 28, 28] \n        x = self.conv4_x(x) # [16, 1024, 14, 14] \n        x = self.conv5_x(x) # [16, 2048, 7, 7] \n        x = self.avg_pool(x) # [16, 2048, 1, 1] \n\n        # reshape할 형상 : (batch_size x 2048)\n        # x = x.view(-1, 2048) # option 1 : view\n        x = torch.flatten(x, 1) # option 2 : flatten \n        # x = x.reshape(x.shape[0], -1) # option 3 : reshape\n\n        x = self.classifier(x)    \n        return x","metadata":{"execution":{"iopub.status.busy":"2023-04-17T02:57:02.714076Z","iopub.execute_input":"2023-04-17T02:57:02.714430Z","iopub.status.idle":"2023-04-17T02:57:02.735767Z","shell.execute_reply.started":"2023-04-17T02:57:02.714396Z","shell.execute_reply":"2023-04-17T02:57:02.734760Z"},"trusted":true},"execution_count":61,"outputs":[]},{"cell_type":"code","source":"model = ResNet50()\nmodel.to(device)\nmodel","metadata":{"execution":{"iopub.status.busy":"2023-04-17T02:57:09.597686Z","iopub.execute_input":"2023-04-17T02:57:09.598376Z","iopub.status.idle":"2023-04-17T02:57:09.835052Z","shell.execute_reply.started":"2023-04-17T02:57:09.598338Z","shell.execute_reply":"2023-04-17T02:57:09.833892Z"},"trusted":true},"execution_count":62,"outputs":[{"execution_count":62,"output_type":"execute_result","data":{"text/plain":"ResNet50(\n  (conv1): Sequential(\n    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): ReLU()\n    (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n  )\n  (shortcut2): Sequential(\n    (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  )\n  (conv2_x): Sequential(\n    (0): ResBlock(\n      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n      (batch_norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (batch_norm2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n      (batch_norm3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (shortcut): Sequential(\n        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (relu): ReLU()\n    )\n    (1): ResBlock(\n      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n      (batch_norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (batch_norm2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n      (batch_norm3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU()\n    )\n    (2): ResBlock(\n      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n      (batch_norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (batch_norm2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n      (batch_norm3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU()\n    )\n  )\n  (shortcut3): Sequential(\n    (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2))\n    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  )\n  (conv3_x): Sequential(\n    (0): ResBlock(\n      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n      (batch_norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n      (batch_norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))\n      (batch_norm3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (shortcut): Sequential(\n        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2))\n        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (relu): ReLU()\n    )\n    (1): ResBlock(\n      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n      (batch_norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (batch_norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))\n      (batch_norm3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU()\n    )\n    (2): ResBlock(\n      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n      (batch_norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (batch_norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))\n      (batch_norm3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU()\n    )\n    (3): ResBlock(\n      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n      (batch_norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (batch_norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))\n      (batch_norm3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU()\n    )\n  )\n  (shortcut4): Sequential(\n    (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2))\n    (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  )\n  (conv4_x): Sequential(\n    (0): ResBlock(\n      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n      (batch_norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n      (batch_norm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))\n      (batch_norm3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (shortcut): Sequential(\n        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2))\n        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (relu): ReLU()\n    )\n    (1): ResBlock(\n      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n      (batch_norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (batch_norm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))\n      (batch_norm3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU()\n    )\n    (2): ResBlock(\n      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n      (batch_norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (batch_norm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))\n      (batch_norm3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU()\n    )\n    (3): ResBlock(\n      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n      (batch_norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (batch_norm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))\n      (batch_norm3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU()\n    )\n    (4): ResBlock(\n      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n      (batch_norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (batch_norm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))\n      (batch_norm3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU()\n    )\n    (5): ResBlock(\n      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n      (batch_norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (batch_norm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))\n      (batch_norm3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU()\n    )\n  )\n  (shortcut5): Sequential(\n    (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2))\n    (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  )\n  (conv5_x): Sequential(\n    (0): ResBlock(\n      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1))\n      (batch_norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n      (batch_norm2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1))\n      (batch_norm3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (shortcut): Sequential(\n        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2))\n        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (relu): ReLU()\n    )\n    (1): ResBlock(\n      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1))\n      (batch_norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (batch_norm2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1))\n      (batch_norm3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU()\n    )\n    (2): ResBlock(\n      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1))\n      (batch_norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (batch_norm2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1))\n      (batch_norm3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU()\n    )\n  )\n  (avg_pool): AdaptiveAvgPool2d(output_size=(1, 1))\n  (classifier): Sequential(\n    (0): Linear(in_features=2048, out_features=120, bias=True)\n  )\n)"},"metadata":{}}]},{"cell_type":"code","source":"out = model(batch['image'].to(device))\nout.shape","metadata":{"execution":{"iopub.status.busy":"2023-04-17T02:57:13.956501Z","iopub.execute_input":"2023-04-17T02:57:13.957215Z","iopub.status.idle":"2023-04-17T02:57:13.983079Z","shell.execute_reply.started":"2023-04-17T02:57:13.957174Z","shell.execute_reply":"2023-04-17T02:57:13.981956Z"},"trusted":true},"execution_count":63,"outputs":[{"execution_count":63,"output_type":"execute_result","data":{"text/plain":"torch.Size([16, 120])"},"metadata":{}}]},{"cell_type":"code","source":"for name, parameter in model.named_parameters():\n    print(name, parameter.size())","metadata":{"execution":{"iopub.status.busy":"2023-04-17T02:43:17.140027Z","iopub.execute_input":"2023-04-17T02:43:17.140697Z","iopub.status.idle":"2023-04-17T02:43:17.150179Z","shell.execute_reply.started":"2023-04-17T02:43:17.140659Z","shell.execute_reply":"2023-04-17T02:43:17.148871Z"},"trusted":true},"execution_count":31,"outputs":[{"name":"stdout","text":"conv1.0.weight torch.Size([64, 3, 7, 7])\nconv1.1.weight torch.Size([64])\nconv1.1.bias torch.Size([64])\nshortcut2.0.weight torch.Size([256, 64, 1, 1])\nshortcut2.0.bias torch.Size([256])\nshortcut2.1.weight torch.Size([256])\nshortcut2.1.bias torch.Size([256])\nconv2_x.0.conv1.weight torch.Size([64, 64, 1, 1])\nconv2_x.0.conv1.bias torch.Size([64])\nconv2_x.0.batch_norm1.weight torch.Size([64])\nconv2_x.0.batch_norm1.bias torch.Size([64])\nconv2_x.0.conv2.weight torch.Size([64, 64, 3, 3])\nconv2_x.0.conv2.bias torch.Size([64])\nconv2_x.0.batch_norm2.weight torch.Size([64])\nconv2_x.0.batch_norm2.bias torch.Size([64])\nconv2_x.0.conv3.weight torch.Size([256, 64, 1, 1])\nconv2_x.0.conv3.bias torch.Size([256])\nconv2_x.0.batch_norm3.weight torch.Size([256])\nconv2_x.0.batch_norm3.bias torch.Size([256])\nconv2_x.1.conv1.weight torch.Size([64, 256, 1, 1])\nconv2_x.1.conv1.bias torch.Size([64])\nconv2_x.1.batch_norm1.weight torch.Size([64])\nconv2_x.1.batch_norm1.bias torch.Size([64])\nconv2_x.1.conv2.weight torch.Size([64, 64, 3, 3])\nconv2_x.1.conv2.bias torch.Size([64])\nconv2_x.1.batch_norm2.weight torch.Size([64])\nconv2_x.1.batch_norm2.bias torch.Size([64])\nconv2_x.1.conv3.weight torch.Size([256, 64, 1, 1])\nconv2_x.1.conv3.bias torch.Size([256])\nconv2_x.1.batch_norm3.weight torch.Size([256])\nconv2_x.1.batch_norm3.bias torch.Size([256])\nconv2_x.2.conv1.weight torch.Size([64, 256, 1, 1])\nconv2_x.2.conv1.bias torch.Size([64])\nconv2_x.2.batch_norm1.weight torch.Size([64])\nconv2_x.2.batch_norm1.bias torch.Size([64])\nconv2_x.2.conv2.weight torch.Size([64, 64, 3, 3])\nconv2_x.2.conv2.bias torch.Size([64])\nconv2_x.2.batch_norm2.weight torch.Size([64])\nconv2_x.2.batch_norm2.bias torch.Size([64])\nconv2_x.2.conv3.weight torch.Size([256, 64, 1, 1])\nconv2_x.2.conv3.bias torch.Size([256])\nconv2_x.2.batch_norm3.weight torch.Size([256])\nconv2_x.2.batch_norm3.bias torch.Size([256])\nshortcut3.0.weight torch.Size([512, 256, 1, 1])\nshortcut3.0.bias torch.Size([512])\nshortcut3.1.weight torch.Size([512])\nshortcut3.1.bias torch.Size([512])\nconv3_x.0.conv1.weight torch.Size([128, 256, 1, 1])\nconv3_x.0.conv1.bias torch.Size([128])\nconv3_x.0.batch_norm1.weight torch.Size([128])\nconv3_x.0.batch_norm1.bias torch.Size([128])\nconv3_x.0.conv2.weight torch.Size([128, 128, 3, 3])\nconv3_x.0.conv2.bias torch.Size([128])\nconv3_x.0.batch_norm2.weight torch.Size([128])\nconv3_x.0.batch_norm2.bias torch.Size([128])\nconv3_x.0.conv3.weight torch.Size([512, 128, 1, 1])\nconv3_x.0.conv3.bias torch.Size([512])\nconv3_x.0.batch_norm3.weight torch.Size([512])\nconv3_x.0.batch_norm3.bias torch.Size([512])\nconv3_x.1.conv1.weight torch.Size([128, 512, 1, 1])\nconv3_x.1.conv1.bias torch.Size([128])\nconv3_x.1.batch_norm1.weight torch.Size([128])\nconv3_x.1.batch_norm1.bias torch.Size([128])\nconv3_x.1.conv2.weight torch.Size([128, 128, 3, 3])\nconv3_x.1.conv2.bias torch.Size([128])\nconv3_x.1.batch_norm2.weight torch.Size([128])\nconv3_x.1.batch_norm2.bias torch.Size([128])\nconv3_x.1.conv3.weight torch.Size([512, 128, 1, 1])\nconv3_x.1.conv3.bias torch.Size([512])\nconv3_x.1.batch_norm3.weight torch.Size([512])\nconv3_x.1.batch_norm3.bias torch.Size([512])\nconv3_x.2.conv1.weight torch.Size([128, 512, 1, 1])\nconv3_x.2.conv1.bias torch.Size([128])\nconv3_x.2.batch_norm1.weight torch.Size([128])\nconv3_x.2.batch_norm1.bias torch.Size([128])\nconv3_x.2.conv2.weight torch.Size([128, 128, 3, 3])\nconv3_x.2.conv2.bias torch.Size([128])\nconv3_x.2.batch_norm2.weight torch.Size([128])\nconv3_x.2.batch_norm2.bias torch.Size([128])\nconv3_x.2.conv3.weight torch.Size([512, 128, 1, 1])\nconv3_x.2.conv3.bias torch.Size([512])\nconv3_x.2.batch_norm3.weight torch.Size([512])\nconv3_x.2.batch_norm3.bias torch.Size([512])\nconv3_x.3.conv1.weight torch.Size([128, 512, 1, 1])\nconv3_x.3.conv1.bias torch.Size([128])\nconv3_x.3.batch_norm1.weight torch.Size([128])\nconv3_x.3.batch_norm1.bias torch.Size([128])\nconv3_x.3.conv2.weight torch.Size([128, 128, 3, 3])\nconv3_x.3.conv2.bias torch.Size([128])\nconv3_x.3.batch_norm2.weight torch.Size([128])\nconv3_x.3.batch_norm2.bias torch.Size([128])\nconv3_x.3.conv3.weight torch.Size([512, 128, 1, 1])\nconv3_x.3.conv3.bias torch.Size([512])\nconv3_x.3.batch_norm3.weight torch.Size([512])\nconv3_x.3.batch_norm3.bias torch.Size([512])\nshortcut4.0.weight torch.Size([1024, 512, 1, 1])\nshortcut4.0.bias torch.Size([1024])\nshortcut4.1.weight torch.Size([1024])\nshortcut4.1.bias torch.Size([1024])\nconv4_x.0.conv1.weight torch.Size([256, 512, 1, 1])\nconv4_x.0.conv1.bias torch.Size([256])\nconv4_x.0.batch_norm1.weight torch.Size([256])\nconv4_x.0.batch_norm1.bias torch.Size([256])\nconv4_x.0.conv2.weight torch.Size([256, 256, 3, 3])\nconv4_x.0.conv2.bias torch.Size([256])\nconv4_x.0.batch_norm2.weight torch.Size([256])\nconv4_x.0.batch_norm2.bias torch.Size([256])\nconv4_x.0.conv3.weight torch.Size([1024, 256, 1, 1])\nconv4_x.0.conv3.bias torch.Size([1024])\nconv4_x.0.batch_norm3.weight torch.Size([1024])\nconv4_x.0.batch_norm3.bias torch.Size([1024])\nconv4_x.1.conv1.weight torch.Size([256, 1024, 1, 1])\nconv4_x.1.conv1.bias torch.Size([256])\nconv4_x.1.batch_norm1.weight torch.Size([256])\nconv4_x.1.batch_norm1.bias torch.Size([256])\nconv4_x.1.conv2.weight torch.Size([256, 256, 3, 3])\nconv4_x.1.conv2.bias torch.Size([256])\nconv4_x.1.batch_norm2.weight torch.Size([256])\nconv4_x.1.batch_norm2.bias torch.Size([256])\nconv4_x.1.conv3.weight torch.Size([1024, 256, 1, 1])\nconv4_x.1.conv3.bias torch.Size([1024])\nconv4_x.1.batch_norm3.weight torch.Size([1024])\nconv4_x.1.batch_norm3.bias torch.Size([1024])\nconv4_x.2.conv1.weight torch.Size([256, 1024, 1, 1])\nconv4_x.2.conv1.bias torch.Size([256])\nconv4_x.2.batch_norm1.weight torch.Size([256])\nconv4_x.2.batch_norm1.bias torch.Size([256])\nconv4_x.2.conv2.weight torch.Size([256, 256, 3, 3])\nconv4_x.2.conv2.bias torch.Size([256])\nconv4_x.2.batch_norm2.weight torch.Size([256])\nconv4_x.2.batch_norm2.bias torch.Size([256])\nconv4_x.2.conv3.weight torch.Size([1024, 256, 1, 1])\nconv4_x.2.conv3.bias torch.Size([1024])\nconv4_x.2.batch_norm3.weight torch.Size([1024])\nconv4_x.2.batch_norm3.bias torch.Size([1024])\nconv4_x.3.conv1.weight torch.Size([256, 1024, 1, 1])\nconv4_x.3.conv1.bias torch.Size([256])\nconv4_x.3.batch_norm1.weight torch.Size([256])\nconv4_x.3.batch_norm1.bias torch.Size([256])\nconv4_x.3.conv2.weight torch.Size([256, 256, 3, 3])\nconv4_x.3.conv2.bias torch.Size([256])\nconv4_x.3.batch_norm2.weight torch.Size([256])\nconv4_x.3.batch_norm2.bias torch.Size([256])\nconv4_x.3.conv3.weight torch.Size([1024, 256, 1, 1])\nconv4_x.3.conv3.bias torch.Size([1024])\nconv4_x.3.batch_norm3.weight torch.Size([1024])\nconv4_x.3.batch_norm3.bias torch.Size([1024])\nconv4_x.4.conv1.weight torch.Size([256, 1024, 1, 1])\nconv4_x.4.conv1.bias torch.Size([256])\nconv4_x.4.batch_norm1.weight torch.Size([256])\nconv4_x.4.batch_norm1.bias torch.Size([256])\nconv4_x.4.conv2.weight torch.Size([256, 256, 3, 3])\nconv4_x.4.conv2.bias torch.Size([256])\nconv4_x.4.batch_norm2.weight torch.Size([256])\nconv4_x.4.batch_norm2.bias torch.Size([256])\nconv4_x.4.conv3.weight torch.Size([1024, 256, 1, 1])\nconv4_x.4.conv3.bias torch.Size([1024])\nconv4_x.4.batch_norm3.weight torch.Size([1024])\nconv4_x.4.batch_norm3.bias torch.Size([1024])\nconv4_x.5.conv1.weight torch.Size([256, 1024, 1, 1])\nconv4_x.5.conv1.bias torch.Size([256])\nconv4_x.5.batch_norm1.weight torch.Size([256])\nconv4_x.5.batch_norm1.bias torch.Size([256])\nconv4_x.5.conv2.weight torch.Size([256, 256, 3, 3])\nconv4_x.5.conv2.bias torch.Size([256])\nconv4_x.5.batch_norm2.weight torch.Size([256])\nconv4_x.5.batch_norm2.bias torch.Size([256])\nconv4_x.5.conv3.weight torch.Size([1024, 256, 1, 1])\nconv4_x.5.conv3.bias torch.Size([1024])\nconv4_x.5.batch_norm3.weight torch.Size([1024])\nconv4_x.5.batch_norm3.bias torch.Size([1024])\nshortcut5.0.weight torch.Size([2048, 1024, 1, 1])\nshortcut5.0.bias torch.Size([2048])\nshortcut5.1.weight torch.Size([2048])\nshortcut5.1.bias torch.Size([2048])\nconv5_x.0.conv1.weight torch.Size([512, 1024, 1, 1])\nconv5_x.0.conv1.bias torch.Size([512])\nconv5_x.0.batch_norm1.weight torch.Size([512])\nconv5_x.0.batch_norm1.bias torch.Size([512])\nconv5_x.0.conv2.weight torch.Size([512, 512, 3, 3])\nconv5_x.0.conv2.bias torch.Size([512])\nconv5_x.0.batch_norm2.weight torch.Size([512])\nconv5_x.0.batch_norm2.bias torch.Size([512])\nconv5_x.0.conv3.weight torch.Size([2048, 512, 1, 1])\nconv5_x.0.conv3.bias torch.Size([2048])\nconv5_x.0.batch_norm3.weight torch.Size([2048])\nconv5_x.0.batch_norm3.bias torch.Size([2048])\nconv5_x.1.conv1.weight torch.Size([512, 2048, 1, 1])\nconv5_x.1.conv1.bias torch.Size([512])\nconv5_x.1.batch_norm1.weight torch.Size([512])\nconv5_x.1.batch_norm1.bias torch.Size([512])\nconv5_x.1.conv2.weight torch.Size([512, 512, 3, 3])\nconv5_x.1.conv2.bias torch.Size([512])\nconv5_x.1.batch_norm2.weight torch.Size([512])\nconv5_x.1.batch_norm2.bias torch.Size([512])\nconv5_x.1.conv3.weight torch.Size([2048, 512, 1, 1])\nconv5_x.1.conv3.bias torch.Size([2048])\nconv5_x.1.batch_norm3.weight torch.Size([2048])\nconv5_x.1.batch_norm3.bias torch.Size([2048])\nconv5_x.2.conv1.weight torch.Size([512, 2048, 1, 1])\nconv5_x.2.conv1.bias torch.Size([512])\nconv5_x.2.batch_norm1.weight torch.Size([512])\nconv5_x.2.batch_norm1.bias torch.Size([512])\nconv5_x.2.conv2.weight torch.Size([512, 512, 3, 3])\nconv5_x.2.conv2.bias torch.Size([512])\nconv5_x.2.batch_norm2.weight torch.Size([512])\nconv5_x.2.batch_norm2.bias torch.Size([512])\nconv5_x.2.conv3.weight torch.Size([2048, 512, 1, 1])\nconv5_x.2.conv3.bias torch.Size([2048])\nconv5_x.2.batch_norm3.weight torch.Size([2048])\nconv5_x.2.batch_norm3.bias torch.Size([2048])\nclassifier.0.weight torch.Size([120, 2048])\nclassifier.0.bias torch.Size([120])\n","output_type":"stream"}]},{"cell_type":"code","source":"learning_rate = 0.0001\n# 손실함수\nloss_fn = nn.CrossEntropyLoss()\n\n# 옵티마이저(최적화함수, 예:경사하강법)\n# optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n\n# 규제의 강도 설정 weight_decay\noptimizer = optim.Adam(model.parameters(), lr=learning_rate)\n# optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=0.001)\n\n# Learning Rate Schedule\n# https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.ReduceLROnPlateau.html\n\n# 모니터링하고 있는 값(예:valid_loss)의 최소값(min) 또는 최대값(max) patience 기간동안 줄어들지 않을 때(OnPlateau) lr에 factor(0.1)를 곱해주는 전략\nscheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=4, verbose=True)","metadata":{"execution":{"iopub.status.busy":"2023-04-17T02:57:18.700905Z","iopub.execute_input":"2023-04-17T02:57:18.701282Z","iopub.status.idle":"2023-04-17T02:57:18.709088Z","shell.execute_reply.started":"2023-04-17T02:57:18.701247Z","shell.execute_reply":"2023-04-17T02:57:18.707996Z"},"trusted":true},"execution_count":64,"outputs":[]},{"cell_type":"code","source":"def validate(model, validloader, loss_fn):\n    total = 0   \n    correct = 0\n    valid_loss = 0\n    valid_accuracy = 0\n\n  # 전방향 예측을 구할 때는 gradient가 필요가 없음음\n    with torch.no_grad():\n        for batch in validloader:# 이터레이터로부터 next()가 호출되며 미니배치를 반환(images, labels)      \n          # images, labels : (torch.Size([16, 3, 224, 224]), torch.Size([16]))\n          # 0. Data를 GPU로 보내기\n            images = batch['image']\n            labels = batch['label']\n            \n            images, labels = images.to(device), labels.to(device)\n            \n            # 1. 입력 데이터 준비\n            # not Flatten !!\n            # images.resize_(images.size()[0], 784)\n  \n            # 2. 전방향(Forward) 예측\n            logit = model(images) # 예측 점수\n            _, preds = torch.max(logit, 1) # 배치에 대한 최종 예측\n            # preds = logit.max(dim=1)[1] \n            correct += int((preds == labels).sum()) # 배치 중 맞은 것의 개수가 correct에 누적\n            total += labels.shape[0] # 배치 사이즈만큼씩 total에 누적\n\n            loss = loss_fn(logit, labels)\n            valid_loss += loss.item() # tensor에서 값을 꺼내와서, 배치의 loss 평균값을 valid_loss에 누적\n\n        valid_accuracy = correct / total\n  \n    return valid_loss, valid_accuracy","metadata":{"execution":{"iopub.status.busy":"2023-04-17T02:57:32.642928Z","iopub.execute_input":"2023-04-17T02:57:32.643289Z","iopub.status.idle":"2023-04-17T02:57:32.651170Z","shell.execute_reply.started":"2023-04-17T02:57:32.643257Z","shell.execute_reply":"2023-04-17T02:57:32.650079Z"},"trusted":true},"execution_count":65,"outputs":[]},{"cell_type":"code","source":"images.shape","metadata":{"execution":{"iopub.status.busy":"2023-04-17T02:57:35.378626Z","iopub.execute_input":"2023-04-17T02:57:35.379327Z","iopub.status.idle":"2023-04-17T02:57:35.400665Z","shell.execute_reply.started":"2023-04-17T02:57:35.379288Z","shell.execute_reply":"2023-04-17T02:57:35.399147Z"},"trusted":true},"execution_count":66,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_24/2810002506.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'images' is not defined"],"ename":"NameError","evalue":"name 'images' is not defined","output_type":"error"}]},{"cell_type":"code","source":" for epoch in range(epochs):\n    model.train() # 훈련 모드\n    train_loss = 0\n    for batch in trainloader: # 이터레이터로부터 next()가 호출되며 미니배치를 반환(images, labels)\n      steps += 1\n      # images, labels : (torch.Size([16, 3, 224, 224]), torch.Size([16]))\n      images = batch['image']\n      key_pts = batch['keypoints']","metadata":{"execution":{"iopub.status.busy":"2023-04-15T14:20:56.226857Z","iopub.execute_input":"2023-04-15T14:20:56.227535Z","iopub.status.idle":"2023-04-15T14:20:56.314630Z","shell.execute_reply.started":"2023-04-15T14:20:56.227487Z","shell.execute_reply":"2023-04-15T14:20:56.313131Z"},"trusted":true},"execution_count":1,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_23/4135914856.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainloader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'trainloader' is not defined"],"ename":"NameError","evalue":"name 'trainloader' is not defined","output_type":"error"}]},{"cell_type":"code","source":"\ndef train_loop(model, trainloader, loss_fn, epochs, optimizer):  \n    steps = 0\n    steps_per_epoch = len(trainloader) \n    min_loss = 1000000\n    max_accuracy = 0\n    trigger = 0\n    patience = 7 \n\n    for epoch in range(epochs):\n        model.train() # 훈련 모드\n        train_loss = 0\n        for batch in trainloader: # 이터레이터로부터 next()가 호출되며 미니배치를 반환(images, labels)\n            steps += 1\n            # images, labels : (torch.Size([16, 3, 224, 224]), torch.Size([16]))\n            # 0. Data를 GPU로 보내기\n            images = batch['image']\n            labels = batch['label']\n            images, labels = images.to(device), labels.to(device)\n\n            # 1. 입력 데이터 준비\n            # not Flatten !!\n            # images.resize_(images.shape[0], 784) \n\n            # 2. 전방향(forward) 예측\n            predict = model(images) # 예측 점수\n            loss = loss_fn(predict, labels) # 예측 점수와 정답을 CrossEntropyLoss에 넣어 Loss값 반환\n\n            # 3. 역방향(backward) 오차(Gradient) 전파\n            optimizer.zero_grad() # Gradient가 누적되지 않게 하기 위해\n            loss.backward() # 모델파리미터들의 Gradient 전파\n\n            # 4. 경사 하강법으로 모델 파라미터 업데이트\n            optimizer.step() # W <- W -lr*Gradient\n\n            train_loss += loss.item()\n            if (steps % steps_per_epoch) == 0 : \n                model.eval() # 평가 모드 : 평가에서 사용하지 않을 계층(배치 정규화, 드롭아웃)들을 수행하지 않게 하기 위해서\n                valid_loss, valid_accuracy = validate(model, validloader, loss_fn)\n            # -------------------------------------------\n\n                print('Epoch : {}/{}.......'.format(epoch+1, epochs),            \n                   'Train Loss : {:.3f}'.format(train_loss/len(trainloader)), \n                   'Valid Loss : {:.3f}'.format(valid_loss/len(validloader)), \n                   'Valid Accuracy : {:.3f}'.format(valid_accuracy)            \n                      )\n\n              # Best model 저장    \n              # option 1 : valid_loss 모니터링\n              # if valid_loss < min_loss: # 바로 이전 epoch의 loss보다 작으면 저장하기\n              #   min_loss = valid_loss\n              #   best_model_state = deepcopy(model.state_dict())          \n              #   torch.save(best_model_state, 'best_checkpoint.pth')     \n\n              # option 2 : valid_accuracy 모니터링      \n                if valid_accuracy > max_accuracy : # 바로 이전 epoch의 accuracy보다 크면 저장하기\n                    max_accuracy = valid_accuracy\n                    best_model_state = deepcopy(model.state_dict())          \n                    torch.save(best_model_state, 'best_checkpoint.pth')  \n              # -------------------------------------------\n\n              # Early Stopping (조기 종료)\n                if valid_loss > min_loss: # valid_loss가 min_loss를 갱신하지 못하면\n                    trigger += 1\n                    print('trigger : ', trigger)\n                if trigger > patience:\n                    print('Early Stopping !!!')\n                    print('Training loop is finished !!')\n                    return\n                else:\n                    trigger = 0\n                    min_loss = valid_loss\n                # -------------------------------------------\n\n                # Learning Rate Scheduler\n                scheduler.step(valid_loss)\n            # -------------------------------------------\n\n    return  ","metadata":{"execution":{"iopub.status.busy":"2023-04-17T02:57:40.864356Z","iopub.execute_input":"2023-04-17T02:57:40.864956Z","iopub.status.idle":"2023-04-17T02:57:40.878439Z","shell.execute_reply.started":"2023-04-17T02:57:40.864914Z","shell.execute_reply":"2023-04-17T02:57:40.877147Z"},"trusted":true},"execution_count":67,"outputs":[]},{"cell_type":"code","source":"epochs = 55\n%time train_loop(model, trainloader, loss_fn, epochs, optimizer)","metadata":{"execution":{"iopub.status.busy":"2023-04-17T02:57:42.940599Z","iopub.execute_input":"2023-04-17T02:57:42.941274Z","iopub.status.idle":"2023-04-17T05:39:04.126561Z","shell.execute_reply.started":"2023-04-17T02:57:42.941237Z","shell.execute_reply":"2023-04-17T05:39:04.125409Z"},"trusted":true},"execution_count":68,"outputs":[{"name":"stdout","text":"Epoch : 1/55....... Train Loss : 4.540 Valid Loss : 4.684 Valid Accuracy : 0.059\nEpoch : 2/55....... Train Loss : 4.022 Valid Loss : 4.136 Valid Accuracy : 0.085\nEpoch : 3/55....... Train Loss : 3.663 Valid Loss : 3.913 Valid Accuracy : 0.126\nEpoch : 4/55....... Train Loss : 3.341 Valid Loss : 3.265 Valid Accuracy : 0.181\nEpoch : 5/55....... Train Loss : 3.026 Valid Loss : 2.997 Valid Accuracy : 0.224\nEpoch : 6/55....... Train Loss : 2.724 Valid Loss : 3.002 Valid Accuracy : 0.241\ntrigger :  1\nEpoch : 7/55....... Train Loss : 2.377 Valid Loss : 2.710 Valid Accuracy : 0.303\nEpoch : 8/55....... Train Loss : 2.048 Valid Loss : 2.691 Valid Accuracy : 0.312\nEpoch : 9/55....... Train Loss : 1.715 Valid Loss : 2.953 Valid Accuracy : 0.305\ntrigger :  1\nEpoch : 10/55....... Train Loss : 1.368 Valid Loss : 3.302 Valid Accuracy : 0.279\ntrigger :  1\nEpoch : 11/55....... Train Loss : 1.055 Valid Loss : 2.716 Valid Accuracy : 0.346\nEpoch : 12/55....... Train Loss : 0.784 Valid Loss : 3.155 Valid Accuracy : 0.317\ntrigger :  1\nEpoch : 13/55....... Train Loss : 0.607 Valid Loss : 3.194 Valid Accuracy : 0.328\ntrigger :  1\nEpoch 00013: reducing learning rate of group 0 to 1.0000e-05.\nEpoch : 14/55....... Train Loss : 0.239 Valid Loss : 2.524 Valid Accuracy : 0.422\nEpoch : 15/55....... Train Loss : 0.139 Valid Loss : 2.599 Valid Accuracy : 0.412\ntrigger :  1\nEpoch : 16/55....... Train Loss : 0.099 Valid Loss : 2.622 Valid Accuracy : 0.413\ntrigger :  1\nEpoch : 17/55....... Train Loss : 0.076 Valid Loss : 2.592 Valid Accuracy : 0.414\nEpoch : 18/55....... Train Loss : 0.061 Valid Loss : 2.644 Valid Accuracy : 0.417\ntrigger :  1\nEpoch : 19/55....... Train Loss : 0.049 Valid Loss : 2.684 Valid Accuracy : 0.408\ntrigger :  1\nEpoch 00019: reducing learning rate of group 0 to 1.0000e-06.\nEpoch : 20/55....... Train Loss : 0.039 Valid Loss : 2.689 Valid Accuracy : 0.409\ntrigger :  1\nEpoch : 21/55....... Train Loss : 0.035 Valid Loss : 2.705 Valid Accuracy : 0.411\ntrigger :  1\nEpoch : 22/55....... Train Loss : 0.034 Valid Loss : 2.690 Valid Accuracy : 0.417\nEpoch : 23/55....... Train Loss : 0.033 Valid Loss : 2.697 Valid Accuracy : 0.414\ntrigger :  1\nEpoch : 24/55....... Train Loss : 0.032 Valid Loss : 2.726 Valid Accuracy : 0.413\ntrigger :  1\nEpoch 00024: reducing learning rate of group 0 to 1.0000e-07.\nEpoch : 25/55....... Train Loss : 0.031 Valid Loss : 2.733 Valid Accuracy : 0.417\ntrigger :  1\nEpoch : 26/55....... Train Loss : 0.032 Valid Loss : 2.708 Valid Accuracy : 0.417\nEpoch : 27/55....... Train Loss : 0.031 Valid Loss : 2.717 Valid Accuracy : 0.417\ntrigger :  1\nEpoch : 28/55....... Train Loss : 0.030 Valid Loss : 2.719 Valid Accuracy : 0.407\ntrigger :  1\nEpoch : 29/55....... Train Loss : 0.029 Valid Loss : 2.704 Valid Accuracy : 0.415\nEpoch 00029: reducing learning rate of group 0 to 1.0000e-08.\nEpoch : 30/55....... Train Loss : 0.029 Valid Loss : 2.706 Valid Accuracy : 0.415\ntrigger :  1\nEpoch : 31/55....... Train Loss : 0.031 Valid Loss : 2.700 Valid Accuracy : 0.418\nEpoch : 32/55....... Train Loss : 0.031 Valid Loss : 2.710 Valid Accuracy : 0.419\ntrigger :  1\nEpoch : 33/55....... Train Loss : 0.028 Valid Loss : 2.704 Valid Accuracy : 0.424\nEpoch : 34/55....... Train Loss : 0.031 Valid Loss : 2.726 Valid Accuracy : 0.414\ntrigger :  1\nEpoch : 35/55....... Train Loss : 0.029 Valid Loss : 2.727 Valid Accuracy : 0.413\ntrigger :  1\nEpoch : 36/55....... Train Loss : 0.032 Valid Loss : 2.719 Valid Accuracy : 0.423\nEpoch : 37/55....... Train Loss : 0.031 Valid Loss : 2.729 Valid Accuracy : 0.418\ntrigger :  1\nEpoch : 38/55....... Train Loss : 0.029 Valid Loss : 2.713 Valid Accuracy : 0.417\nEpoch : 39/55....... Train Loss : 0.030 Valid Loss : 2.705 Valid Accuracy : 0.414\nEpoch : 40/55....... Train Loss : 0.030 Valid Loss : 2.739 Valid Accuracy : 0.410\ntrigger :  1\nEpoch : 41/55....... Train Loss : 0.030 Valid Loss : 2.742 Valid Accuracy : 0.413\ntrigger :  1\nEpoch : 42/55....... Train Loss : 0.031 Valid Loss : 2.736 Valid Accuracy : 0.414\nEpoch : 43/55....... Train Loss : 0.030 Valid Loss : 2.724 Valid Accuracy : 0.409\nEpoch : 44/55....... Train Loss : 0.029 Valid Loss : 2.689 Valid Accuracy : 0.417\nEpoch : 45/55....... Train Loss : 0.028 Valid Loss : 2.687 Valid Accuracy : 0.415\nEpoch : 46/55....... Train Loss : 0.032 Valid Loss : 2.755 Valid Accuracy : 0.411\ntrigger :  1\nEpoch : 47/55....... Train Loss : 0.028 Valid Loss : 2.725 Valid Accuracy : 0.411\nEpoch : 48/55....... Train Loss : 0.031 Valid Loss : 2.738 Valid Accuracy : 0.417\ntrigger :  1\nEpoch : 49/55....... Train Loss : 0.030 Valid Loss : 2.735 Valid Accuracy : 0.414\nEpoch : 50/55....... Train Loss : 0.029 Valid Loss : 2.720 Valid Accuracy : 0.418\nEpoch : 51/55....... Train Loss : 0.028 Valid Loss : 2.692 Valid Accuracy : 0.414\nEpoch : 52/55....... Train Loss : 0.029 Valid Loss : 2.735 Valid Accuracy : 0.421\ntrigger :  1\nEpoch : 53/55....... Train Loss : 0.029 Valid Loss : 2.721 Valid Accuracy : 0.414\nEpoch : 54/55....... Train Loss : 0.030 Valid Loss : 2.691 Valid Accuracy : 0.413\nEpoch : 55/55....... Train Loss : 0.029 Valid Loss : 2.735 Valid Accuracy : 0.411\ntrigger :  1\nCPU times: user 2h 15min 18s, sys: 29min 48s, total: 2h 45min 7s\nWall time: 2h 41min 21s\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## 모델 예측 ","metadata":{}},{"cell_type":"code","source":"test_iter = iter(testloader)\nbatch = next(test_iter)\nimages = batch['image']\nlabels = batch['label']\n\nimages, labels = images.to(device), labels.to(device)\nprint(images.size(), labels.size())","metadata":{"execution":{"iopub.status.busy":"2023-04-17T05:42:00.002072Z","iopub.execute_input":"2023-04-17T05:42:00.002445Z","iopub.status.idle":"2023-04-17T05:42:00.092906Z","shell.execute_reply.started":"2023-04-17T05:42:00.002411Z","shell.execute_reply":"2023-04-17T05:42:00.091789Z"},"trusted":true},"execution_count":70,"outputs":[{"name":"stdout","text":"torch.Size([16, 3, 224, 224]) torch.Size([16])\n","output_type":"stream"}]},{"cell_type":"code","source":"def evaluation(model, testloader, loss_fn):\n    total = 0   \n    correct = 0\n    test_loss = 0\n    test_accuracy = 0\n\n  # 전방향 예측을 구할 때는 gradient가 필요가 없음음\n    with torch.no_grad():\n        for images, labels in testloader: # 이터레이터로부터 next()가 호출되며 미니배치를 반환(images, labels)\n          # 0. Data를 GPU로 보내기\n            images = batch['image']\n            labels = batch['label']\n            images, labels = images.to(device), labels.to(device)\n          # 1. 입력 데이터 준비\n          # not Flatten\n          # images.resize_(images.size()[0], 784)\n      \n          # 2. 전방향(Forward) 예측\n            logit = model(images) # 예측 점수\n            _, preds = torch.max(logit, 1) # 배치에 대한 최종 예측\n            # preds = logit.max(dim=1)[1] \n            correct += int((preds == labels).sum()) # 배치치 중 맞은 것의 개수가 correct에 누적\n            total += labels.shape[0] # 배치 사이즈만큼씩 total에 누적\n\n            loss = loss_fn(logit, labels)\n            test_loss += loss.item() # tensor에서 값을 꺼내와서, 배치의 loss 평균값을 valid_loss에 누적\n        \n        test_accuracy = correct / total\n   \n        print('Test Loss : {:.3f}'.format(test_loss/len(testloader)), \n        'Test Accuracy : {:.3f}'.format(test_accuracy))\n\nmodel.eval()\nevaluation(model, testloader, loss_fn)  ","metadata":{"execution":{"iopub.status.busy":"2023-04-17T05:43:51.146539Z","iopub.execute_input":"2023-04-17T05:43:51.146931Z","iopub.status.idle":"2023-04-17T05:44:28.177412Z","shell.execute_reply.started":"2023-04-17T05:43:51.146894Z","shell.execute_reply":"2023-04-17T05:44:28.176324Z"},"trusted":true},"execution_count":76,"outputs":[{"name":"stdout","text":"Test Loss : 1.320 Test Accuracy : 0.625\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## 모델 저장","metadata":{}},{"cell_type":"code","source":"# 현재 모델에 저장되어 있는 모델 파라미터터\nmodel.state_dict().keys()","metadata":{"execution":{"iopub.status.busy":"2023-04-17T05:45:49.128661Z","iopub.execute_input":"2023-04-17T05:45:49.129044Z","iopub.status.idle":"2023-04-17T05:45:49.140901Z","shell.execute_reply.started":"2023-04-17T05:45:49.129012Z","shell.execute_reply":"2023-04-17T05:45:49.139785Z"},"trusted":true},"execution_count":77,"outputs":[{"execution_count":77,"output_type":"execute_result","data":{"text/plain":"odict_keys(['conv1.0.weight', 'conv1.1.weight', 'conv1.1.bias', 'conv1.1.running_mean', 'conv1.1.running_var', 'conv1.1.num_batches_tracked', 'shortcut2.0.weight', 'shortcut2.0.bias', 'shortcut2.1.weight', 'shortcut2.1.bias', 'shortcut2.1.running_mean', 'shortcut2.1.running_var', 'shortcut2.1.num_batches_tracked', 'conv2_x.0.conv1.weight', 'conv2_x.0.conv1.bias', 'conv2_x.0.batch_norm1.weight', 'conv2_x.0.batch_norm1.bias', 'conv2_x.0.batch_norm1.running_mean', 'conv2_x.0.batch_norm1.running_var', 'conv2_x.0.batch_norm1.num_batches_tracked', 'conv2_x.0.conv2.weight', 'conv2_x.0.conv2.bias', 'conv2_x.0.batch_norm2.weight', 'conv2_x.0.batch_norm2.bias', 'conv2_x.0.batch_norm2.running_mean', 'conv2_x.0.batch_norm2.running_var', 'conv2_x.0.batch_norm2.num_batches_tracked', 'conv2_x.0.conv3.weight', 'conv2_x.0.conv3.bias', 'conv2_x.0.batch_norm3.weight', 'conv2_x.0.batch_norm3.bias', 'conv2_x.0.batch_norm3.running_mean', 'conv2_x.0.batch_norm3.running_var', 'conv2_x.0.batch_norm3.num_batches_tracked', 'conv2_x.0.shortcut.0.weight', 'conv2_x.0.shortcut.0.bias', 'conv2_x.0.shortcut.1.weight', 'conv2_x.0.shortcut.1.bias', 'conv2_x.0.shortcut.1.running_mean', 'conv2_x.0.shortcut.1.running_var', 'conv2_x.0.shortcut.1.num_batches_tracked', 'conv2_x.1.conv1.weight', 'conv2_x.1.conv1.bias', 'conv2_x.1.batch_norm1.weight', 'conv2_x.1.batch_norm1.bias', 'conv2_x.1.batch_norm1.running_mean', 'conv2_x.1.batch_norm1.running_var', 'conv2_x.1.batch_norm1.num_batches_tracked', 'conv2_x.1.conv2.weight', 'conv2_x.1.conv2.bias', 'conv2_x.1.batch_norm2.weight', 'conv2_x.1.batch_norm2.bias', 'conv2_x.1.batch_norm2.running_mean', 'conv2_x.1.batch_norm2.running_var', 'conv2_x.1.batch_norm2.num_batches_tracked', 'conv2_x.1.conv3.weight', 'conv2_x.1.conv3.bias', 'conv2_x.1.batch_norm3.weight', 'conv2_x.1.batch_norm3.bias', 'conv2_x.1.batch_norm3.running_mean', 'conv2_x.1.batch_norm3.running_var', 'conv2_x.1.batch_norm3.num_batches_tracked', 'conv2_x.2.conv1.weight', 'conv2_x.2.conv1.bias', 'conv2_x.2.batch_norm1.weight', 'conv2_x.2.batch_norm1.bias', 'conv2_x.2.batch_norm1.running_mean', 'conv2_x.2.batch_norm1.running_var', 'conv2_x.2.batch_norm1.num_batches_tracked', 'conv2_x.2.conv2.weight', 'conv2_x.2.conv2.bias', 'conv2_x.2.batch_norm2.weight', 'conv2_x.2.batch_norm2.bias', 'conv2_x.2.batch_norm2.running_mean', 'conv2_x.2.batch_norm2.running_var', 'conv2_x.2.batch_norm2.num_batches_tracked', 'conv2_x.2.conv3.weight', 'conv2_x.2.conv3.bias', 'conv2_x.2.batch_norm3.weight', 'conv2_x.2.batch_norm3.bias', 'conv2_x.2.batch_norm3.running_mean', 'conv2_x.2.batch_norm3.running_var', 'conv2_x.2.batch_norm3.num_batches_tracked', 'shortcut3.0.weight', 'shortcut3.0.bias', 'shortcut3.1.weight', 'shortcut3.1.bias', 'shortcut3.1.running_mean', 'shortcut3.1.running_var', 'shortcut3.1.num_batches_tracked', 'conv3_x.0.conv1.weight', 'conv3_x.0.conv1.bias', 'conv3_x.0.batch_norm1.weight', 'conv3_x.0.batch_norm1.bias', 'conv3_x.0.batch_norm1.running_mean', 'conv3_x.0.batch_norm1.running_var', 'conv3_x.0.batch_norm1.num_batches_tracked', 'conv3_x.0.conv2.weight', 'conv3_x.0.conv2.bias', 'conv3_x.0.batch_norm2.weight', 'conv3_x.0.batch_norm2.bias', 'conv3_x.0.batch_norm2.running_mean', 'conv3_x.0.batch_norm2.running_var', 'conv3_x.0.batch_norm2.num_batches_tracked', 'conv3_x.0.conv3.weight', 'conv3_x.0.conv3.bias', 'conv3_x.0.batch_norm3.weight', 'conv3_x.0.batch_norm3.bias', 'conv3_x.0.batch_norm3.running_mean', 'conv3_x.0.batch_norm3.running_var', 'conv3_x.0.batch_norm3.num_batches_tracked', 'conv3_x.0.shortcut.0.weight', 'conv3_x.0.shortcut.0.bias', 'conv3_x.0.shortcut.1.weight', 'conv3_x.0.shortcut.1.bias', 'conv3_x.0.shortcut.1.running_mean', 'conv3_x.0.shortcut.1.running_var', 'conv3_x.0.shortcut.1.num_batches_tracked', 'conv3_x.1.conv1.weight', 'conv3_x.1.conv1.bias', 'conv3_x.1.batch_norm1.weight', 'conv3_x.1.batch_norm1.bias', 'conv3_x.1.batch_norm1.running_mean', 'conv3_x.1.batch_norm1.running_var', 'conv3_x.1.batch_norm1.num_batches_tracked', 'conv3_x.1.conv2.weight', 'conv3_x.1.conv2.bias', 'conv3_x.1.batch_norm2.weight', 'conv3_x.1.batch_norm2.bias', 'conv3_x.1.batch_norm2.running_mean', 'conv3_x.1.batch_norm2.running_var', 'conv3_x.1.batch_norm2.num_batches_tracked', 'conv3_x.1.conv3.weight', 'conv3_x.1.conv3.bias', 'conv3_x.1.batch_norm3.weight', 'conv3_x.1.batch_norm3.bias', 'conv3_x.1.batch_norm3.running_mean', 'conv3_x.1.batch_norm3.running_var', 'conv3_x.1.batch_norm3.num_batches_tracked', 'conv3_x.2.conv1.weight', 'conv3_x.2.conv1.bias', 'conv3_x.2.batch_norm1.weight', 'conv3_x.2.batch_norm1.bias', 'conv3_x.2.batch_norm1.running_mean', 'conv3_x.2.batch_norm1.running_var', 'conv3_x.2.batch_norm1.num_batches_tracked', 'conv3_x.2.conv2.weight', 'conv3_x.2.conv2.bias', 'conv3_x.2.batch_norm2.weight', 'conv3_x.2.batch_norm2.bias', 'conv3_x.2.batch_norm2.running_mean', 'conv3_x.2.batch_norm2.running_var', 'conv3_x.2.batch_norm2.num_batches_tracked', 'conv3_x.2.conv3.weight', 'conv3_x.2.conv3.bias', 'conv3_x.2.batch_norm3.weight', 'conv3_x.2.batch_norm3.bias', 'conv3_x.2.batch_norm3.running_mean', 'conv3_x.2.batch_norm3.running_var', 'conv3_x.2.batch_norm3.num_batches_tracked', 'conv3_x.3.conv1.weight', 'conv3_x.3.conv1.bias', 'conv3_x.3.batch_norm1.weight', 'conv3_x.3.batch_norm1.bias', 'conv3_x.3.batch_norm1.running_mean', 'conv3_x.3.batch_norm1.running_var', 'conv3_x.3.batch_norm1.num_batches_tracked', 'conv3_x.3.conv2.weight', 'conv3_x.3.conv2.bias', 'conv3_x.3.batch_norm2.weight', 'conv3_x.3.batch_norm2.bias', 'conv3_x.3.batch_norm2.running_mean', 'conv3_x.3.batch_norm2.running_var', 'conv3_x.3.batch_norm2.num_batches_tracked', 'conv3_x.3.conv3.weight', 'conv3_x.3.conv3.bias', 'conv3_x.3.batch_norm3.weight', 'conv3_x.3.batch_norm3.bias', 'conv3_x.3.batch_norm3.running_mean', 'conv3_x.3.batch_norm3.running_var', 'conv3_x.3.batch_norm3.num_batches_tracked', 'shortcut4.0.weight', 'shortcut4.0.bias', 'shortcut4.1.weight', 'shortcut4.1.bias', 'shortcut4.1.running_mean', 'shortcut4.1.running_var', 'shortcut4.1.num_batches_tracked', 'conv4_x.0.conv1.weight', 'conv4_x.0.conv1.bias', 'conv4_x.0.batch_norm1.weight', 'conv4_x.0.batch_norm1.bias', 'conv4_x.0.batch_norm1.running_mean', 'conv4_x.0.batch_norm1.running_var', 'conv4_x.0.batch_norm1.num_batches_tracked', 'conv4_x.0.conv2.weight', 'conv4_x.0.conv2.bias', 'conv4_x.0.batch_norm2.weight', 'conv4_x.0.batch_norm2.bias', 'conv4_x.0.batch_norm2.running_mean', 'conv4_x.0.batch_norm2.running_var', 'conv4_x.0.batch_norm2.num_batches_tracked', 'conv4_x.0.conv3.weight', 'conv4_x.0.conv3.bias', 'conv4_x.0.batch_norm3.weight', 'conv4_x.0.batch_norm3.bias', 'conv4_x.0.batch_norm3.running_mean', 'conv4_x.0.batch_norm3.running_var', 'conv4_x.0.batch_norm3.num_batches_tracked', 'conv4_x.0.shortcut.0.weight', 'conv4_x.0.shortcut.0.bias', 'conv4_x.0.shortcut.1.weight', 'conv4_x.0.shortcut.1.bias', 'conv4_x.0.shortcut.1.running_mean', 'conv4_x.0.shortcut.1.running_var', 'conv4_x.0.shortcut.1.num_batches_tracked', 'conv4_x.1.conv1.weight', 'conv4_x.1.conv1.bias', 'conv4_x.1.batch_norm1.weight', 'conv4_x.1.batch_norm1.bias', 'conv4_x.1.batch_norm1.running_mean', 'conv4_x.1.batch_norm1.running_var', 'conv4_x.1.batch_norm1.num_batches_tracked', 'conv4_x.1.conv2.weight', 'conv4_x.1.conv2.bias', 'conv4_x.1.batch_norm2.weight', 'conv4_x.1.batch_norm2.bias', 'conv4_x.1.batch_norm2.running_mean', 'conv4_x.1.batch_norm2.running_var', 'conv4_x.1.batch_norm2.num_batches_tracked', 'conv4_x.1.conv3.weight', 'conv4_x.1.conv3.bias', 'conv4_x.1.batch_norm3.weight', 'conv4_x.1.batch_norm3.bias', 'conv4_x.1.batch_norm3.running_mean', 'conv4_x.1.batch_norm3.running_var', 'conv4_x.1.batch_norm3.num_batches_tracked', 'conv4_x.2.conv1.weight', 'conv4_x.2.conv1.bias', 'conv4_x.2.batch_norm1.weight', 'conv4_x.2.batch_norm1.bias', 'conv4_x.2.batch_norm1.running_mean', 'conv4_x.2.batch_norm1.running_var', 'conv4_x.2.batch_norm1.num_batches_tracked', 'conv4_x.2.conv2.weight', 'conv4_x.2.conv2.bias', 'conv4_x.2.batch_norm2.weight', 'conv4_x.2.batch_norm2.bias', 'conv4_x.2.batch_norm2.running_mean', 'conv4_x.2.batch_norm2.running_var', 'conv4_x.2.batch_norm2.num_batches_tracked', 'conv4_x.2.conv3.weight', 'conv4_x.2.conv3.bias', 'conv4_x.2.batch_norm3.weight', 'conv4_x.2.batch_norm3.bias', 'conv4_x.2.batch_norm3.running_mean', 'conv4_x.2.batch_norm3.running_var', 'conv4_x.2.batch_norm3.num_batches_tracked', 'conv4_x.3.conv1.weight', 'conv4_x.3.conv1.bias', 'conv4_x.3.batch_norm1.weight', 'conv4_x.3.batch_norm1.bias', 'conv4_x.3.batch_norm1.running_mean', 'conv4_x.3.batch_norm1.running_var', 'conv4_x.3.batch_norm1.num_batches_tracked', 'conv4_x.3.conv2.weight', 'conv4_x.3.conv2.bias', 'conv4_x.3.batch_norm2.weight', 'conv4_x.3.batch_norm2.bias', 'conv4_x.3.batch_norm2.running_mean', 'conv4_x.3.batch_norm2.running_var', 'conv4_x.3.batch_norm2.num_batches_tracked', 'conv4_x.3.conv3.weight', 'conv4_x.3.conv3.bias', 'conv4_x.3.batch_norm3.weight', 'conv4_x.3.batch_norm3.bias', 'conv4_x.3.batch_norm3.running_mean', 'conv4_x.3.batch_norm3.running_var', 'conv4_x.3.batch_norm3.num_batches_tracked', 'conv4_x.4.conv1.weight', 'conv4_x.4.conv1.bias', 'conv4_x.4.batch_norm1.weight', 'conv4_x.4.batch_norm1.bias', 'conv4_x.4.batch_norm1.running_mean', 'conv4_x.4.batch_norm1.running_var', 'conv4_x.4.batch_norm1.num_batches_tracked', 'conv4_x.4.conv2.weight', 'conv4_x.4.conv2.bias', 'conv4_x.4.batch_norm2.weight', 'conv4_x.4.batch_norm2.bias', 'conv4_x.4.batch_norm2.running_mean', 'conv4_x.4.batch_norm2.running_var', 'conv4_x.4.batch_norm2.num_batches_tracked', 'conv4_x.4.conv3.weight', 'conv4_x.4.conv3.bias', 'conv4_x.4.batch_norm3.weight', 'conv4_x.4.batch_norm3.bias', 'conv4_x.4.batch_norm3.running_mean', 'conv4_x.4.batch_norm3.running_var', 'conv4_x.4.batch_norm3.num_batches_tracked', 'conv4_x.5.conv1.weight', 'conv4_x.5.conv1.bias', 'conv4_x.5.batch_norm1.weight', 'conv4_x.5.batch_norm1.bias', 'conv4_x.5.batch_norm1.running_mean', 'conv4_x.5.batch_norm1.running_var', 'conv4_x.5.batch_norm1.num_batches_tracked', 'conv4_x.5.conv2.weight', 'conv4_x.5.conv2.bias', 'conv4_x.5.batch_norm2.weight', 'conv4_x.5.batch_norm2.bias', 'conv4_x.5.batch_norm2.running_mean', 'conv4_x.5.batch_norm2.running_var', 'conv4_x.5.batch_norm2.num_batches_tracked', 'conv4_x.5.conv3.weight', 'conv4_x.5.conv3.bias', 'conv4_x.5.batch_norm3.weight', 'conv4_x.5.batch_norm3.bias', 'conv4_x.5.batch_norm3.running_mean', 'conv4_x.5.batch_norm3.running_var', 'conv4_x.5.batch_norm3.num_batches_tracked', 'shortcut5.0.weight', 'shortcut5.0.bias', 'shortcut5.1.weight', 'shortcut5.1.bias', 'shortcut5.1.running_mean', 'shortcut5.1.running_var', 'shortcut5.1.num_batches_tracked', 'conv5_x.0.conv1.weight', 'conv5_x.0.conv1.bias', 'conv5_x.0.batch_norm1.weight', 'conv5_x.0.batch_norm1.bias', 'conv5_x.0.batch_norm1.running_mean', 'conv5_x.0.batch_norm1.running_var', 'conv5_x.0.batch_norm1.num_batches_tracked', 'conv5_x.0.conv2.weight', 'conv5_x.0.conv2.bias', 'conv5_x.0.batch_norm2.weight', 'conv5_x.0.batch_norm2.bias', 'conv5_x.0.batch_norm2.running_mean', 'conv5_x.0.batch_norm2.running_var', 'conv5_x.0.batch_norm2.num_batches_tracked', 'conv5_x.0.conv3.weight', 'conv5_x.0.conv3.bias', 'conv5_x.0.batch_norm3.weight', 'conv5_x.0.batch_norm3.bias', 'conv5_x.0.batch_norm3.running_mean', 'conv5_x.0.batch_norm3.running_var', 'conv5_x.0.batch_norm3.num_batches_tracked', 'conv5_x.0.shortcut.0.weight', 'conv5_x.0.shortcut.0.bias', 'conv5_x.0.shortcut.1.weight', 'conv5_x.0.shortcut.1.bias', 'conv5_x.0.shortcut.1.running_mean', 'conv5_x.0.shortcut.1.running_var', 'conv5_x.0.shortcut.1.num_batches_tracked', 'conv5_x.1.conv1.weight', 'conv5_x.1.conv1.bias', 'conv5_x.1.batch_norm1.weight', 'conv5_x.1.batch_norm1.bias', 'conv5_x.1.batch_norm1.running_mean', 'conv5_x.1.batch_norm1.running_var', 'conv5_x.1.batch_norm1.num_batches_tracked', 'conv5_x.1.conv2.weight', 'conv5_x.1.conv2.bias', 'conv5_x.1.batch_norm2.weight', 'conv5_x.1.batch_norm2.bias', 'conv5_x.1.batch_norm2.running_mean', 'conv5_x.1.batch_norm2.running_var', 'conv5_x.1.batch_norm2.num_batches_tracked', 'conv5_x.1.conv3.weight', 'conv5_x.1.conv3.bias', 'conv5_x.1.batch_norm3.weight', 'conv5_x.1.batch_norm3.bias', 'conv5_x.1.batch_norm3.running_mean', 'conv5_x.1.batch_norm3.running_var', 'conv5_x.1.batch_norm3.num_batches_tracked', 'conv5_x.2.conv1.weight', 'conv5_x.2.conv1.bias', 'conv5_x.2.batch_norm1.weight', 'conv5_x.2.batch_norm1.bias', 'conv5_x.2.batch_norm1.running_mean', 'conv5_x.2.batch_norm1.running_var', 'conv5_x.2.batch_norm1.num_batches_tracked', 'conv5_x.2.conv2.weight', 'conv5_x.2.conv2.bias', 'conv5_x.2.batch_norm2.weight', 'conv5_x.2.batch_norm2.bias', 'conv5_x.2.batch_norm2.running_mean', 'conv5_x.2.batch_norm2.running_var', 'conv5_x.2.batch_norm2.num_batches_tracked', 'conv5_x.2.conv3.weight', 'conv5_x.2.conv3.bias', 'conv5_x.2.batch_norm3.weight', 'conv5_x.2.batch_norm3.bias', 'conv5_x.2.batch_norm3.running_mean', 'conv5_x.2.batch_norm3.running_var', 'conv5_x.2.batch_norm3.num_batches_tracked', 'classifier.0.weight', 'classifier.0.bias'])"},"metadata":{}}]},{"cell_type":"code","source":"torch.save(model.state_dict(), 'last_checkpoint.pth')","metadata":{"execution":{"iopub.status.busy":"2023-04-17T05:46:28.105331Z","iopub.execute_input":"2023-04-17T05:46:28.106331Z","iopub.status.idle":"2023-04-17T05:46:28.270589Z","shell.execute_reply.started":"2023-04-17T05:46:28.106290Z","shell.execute_reply":"2023-04-17T05:46:28.269551Z"},"trusted":true},"execution_count":78,"outputs":[]},{"cell_type":"code","source":"# 시간이 흐른뒤 다시 모델 가져오기\nlast_state_dict = torch.load('last_checkpoint.pth')","metadata":{"execution":{"iopub.status.busy":"2023-04-17T05:46:36.498745Z","iopub.execute_input":"2023-04-17T05:46:36.499102Z","iopub.status.idle":"2023-04-17T05:46:36.594041Z","shell.execute_reply.started":"2023-04-17T05:46:36.499071Z","shell.execute_reply":"2023-04-17T05:46:36.593024Z"},"trusted":true},"execution_count":79,"outputs":[]},{"cell_type":"code","source":"last_state_dict.keys()","metadata":{"execution":{"iopub.status.busy":"2023-04-17T05:46:47.733043Z","iopub.execute_input":"2023-04-17T05:46:47.733775Z","iopub.status.idle":"2023-04-17T05:46:47.741919Z","shell.execute_reply.started":"2023-04-17T05:46:47.733728Z","shell.execute_reply":"2023-04-17T05:46:47.740788Z"},"trusted":true},"execution_count":80,"outputs":[{"execution_count":80,"output_type":"execute_result","data":{"text/plain":"odict_keys(['conv1.0.weight', 'conv1.1.weight', 'conv1.1.bias', 'conv1.1.running_mean', 'conv1.1.running_var', 'conv1.1.num_batches_tracked', 'shortcut2.0.weight', 'shortcut2.0.bias', 'shortcut2.1.weight', 'shortcut2.1.bias', 'shortcut2.1.running_mean', 'shortcut2.1.running_var', 'shortcut2.1.num_batches_tracked', 'conv2_x.0.conv1.weight', 'conv2_x.0.conv1.bias', 'conv2_x.0.batch_norm1.weight', 'conv2_x.0.batch_norm1.bias', 'conv2_x.0.batch_norm1.running_mean', 'conv2_x.0.batch_norm1.running_var', 'conv2_x.0.batch_norm1.num_batches_tracked', 'conv2_x.0.conv2.weight', 'conv2_x.0.conv2.bias', 'conv2_x.0.batch_norm2.weight', 'conv2_x.0.batch_norm2.bias', 'conv2_x.0.batch_norm2.running_mean', 'conv2_x.0.batch_norm2.running_var', 'conv2_x.0.batch_norm2.num_batches_tracked', 'conv2_x.0.conv3.weight', 'conv2_x.0.conv3.bias', 'conv2_x.0.batch_norm3.weight', 'conv2_x.0.batch_norm3.bias', 'conv2_x.0.batch_norm3.running_mean', 'conv2_x.0.batch_norm3.running_var', 'conv2_x.0.batch_norm3.num_batches_tracked', 'conv2_x.0.shortcut.0.weight', 'conv2_x.0.shortcut.0.bias', 'conv2_x.0.shortcut.1.weight', 'conv2_x.0.shortcut.1.bias', 'conv2_x.0.shortcut.1.running_mean', 'conv2_x.0.shortcut.1.running_var', 'conv2_x.0.shortcut.1.num_batches_tracked', 'conv2_x.1.conv1.weight', 'conv2_x.1.conv1.bias', 'conv2_x.1.batch_norm1.weight', 'conv2_x.1.batch_norm1.bias', 'conv2_x.1.batch_norm1.running_mean', 'conv2_x.1.batch_norm1.running_var', 'conv2_x.1.batch_norm1.num_batches_tracked', 'conv2_x.1.conv2.weight', 'conv2_x.1.conv2.bias', 'conv2_x.1.batch_norm2.weight', 'conv2_x.1.batch_norm2.bias', 'conv2_x.1.batch_norm2.running_mean', 'conv2_x.1.batch_norm2.running_var', 'conv2_x.1.batch_norm2.num_batches_tracked', 'conv2_x.1.conv3.weight', 'conv2_x.1.conv3.bias', 'conv2_x.1.batch_norm3.weight', 'conv2_x.1.batch_norm3.bias', 'conv2_x.1.batch_norm3.running_mean', 'conv2_x.1.batch_norm3.running_var', 'conv2_x.1.batch_norm3.num_batches_tracked', 'conv2_x.2.conv1.weight', 'conv2_x.2.conv1.bias', 'conv2_x.2.batch_norm1.weight', 'conv2_x.2.batch_norm1.bias', 'conv2_x.2.batch_norm1.running_mean', 'conv2_x.2.batch_norm1.running_var', 'conv2_x.2.batch_norm1.num_batches_tracked', 'conv2_x.2.conv2.weight', 'conv2_x.2.conv2.bias', 'conv2_x.2.batch_norm2.weight', 'conv2_x.2.batch_norm2.bias', 'conv2_x.2.batch_norm2.running_mean', 'conv2_x.2.batch_norm2.running_var', 'conv2_x.2.batch_norm2.num_batches_tracked', 'conv2_x.2.conv3.weight', 'conv2_x.2.conv3.bias', 'conv2_x.2.batch_norm3.weight', 'conv2_x.2.batch_norm3.bias', 'conv2_x.2.batch_norm3.running_mean', 'conv2_x.2.batch_norm3.running_var', 'conv2_x.2.batch_norm3.num_batches_tracked', 'shortcut3.0.weight', 'shortcut3.0.bias', 'shortcut3.1.weight', 'shortcut3.1.bias', 'shortcut3.1.running_mean', 'shortcut3.1.running_var', 'shortcut3.1.num_batches_tracked', 'conv3_x.0.conv1.weight', 'conv3_x.0.conv1.bias', 'conv3_x.0.batch_norm1.weight', 'conv3_x.0.batch_norm1.bias', 'conv3_x.0.batch_norm1.running_mean', 'conv3_x.0.batch_norm1.running_var', 'conv3_x.0.batch_norm1.num_batches_tracked', 'conv3_x.0.conv2.weight', 'conv3_x.0.conv2.bias', 'conv3_x.0.batch_norm2.weight', 'conv3_x.0.batch_norm2.bias', 'conv3_x.0.batch_norm2.running_mean', 'conv3_x.0.batch_norm2.running_var', 'conv3_x.0.batch_norm2.num_batches_tracked', 'conv3_x.0.conv3.weight', 'conv3_x.0.conv3.bias', 'conv3_x.0.batch_norm3.weight', 'conv3_x.0.batch_norm3.bias', 'conv3_x.0.batch_norm3.running_mean', 'conv3_x.0.batch_norm3.running_var', 'conv3_x.0.batch_norm3.num_batches_tracked', 'conv3_x.0.shortcut.0.weight', 'conv3_x.0.shortcut.0.bias', 'conv3_x.0.shortcut.1.weight', 'conv3_x.0.shortcut.1.bias', 'conv3_x.0.shortcut.1.running_mean', 'conv3_x.0.shortcut.1.running_var', 'conv3_x.0.shortcut.1.num_batches_tracked', 'conv3_x.1.conv1.weight', 'conv3_x.1.conv1.bias', 'conv3_x.1.batch_norm1.weight', 'conv3_x.1.batch_norm1.bias', 'conv3_x.1.batch_norm1.running_mean', 'conv3_x.1.batch_norm1.running_var', 'conv3_x.1.batch_norm1.num_batches_tracked', 'conv3_x.1.conv2.weight', 'conv3_x.1.conv2.bias', 'conv3_x.1.batch_norm2.weight', 'conv3_x.1.batch_norm2.bias', 'conv3_x.1.batch_norm2.running_mean', 'conv3_x.1.batch_norm2.running_var', 'conv3_x.1.batch_norm2.num_batches_tracked', 'conv3_x.1.conv3.weight', 'conv3_x.1.conv3.bias', 'conv3_x.1.batch_norm3.weight', 'conv3_x.1.batch_norm3.bias', 'conv3_x.1.batch_norm3.running_mean', 'conv3_x.1.batch_norm3.running_var', 'conv3_x.1.batch_norm3.num_batches_tracked', 'conv3_x.2.conv1.weight', 'conv3_x.2.conv1.bias', 'conv3_x.2.batch_norm1.weight', 'conv3_x.2.batch_norm1.bias', 'conv3_x.2.batch_norm1.running_mean', 'conv3_x.2.batch_norm1.running_var', 'conv3_x.2.batch_norm1.num_batches_tracked', 'conv3_x.2.conv2.weight', 'conv3_x.2.conv2.bias', 'conv3_x.2.batch_norm2.weight', 'conv3_x.2.batch_norm2.bias', 'conv3_x.2.batch_norm2.running_mean', 'conv3_x.2.batch_norm2.running_var', 'conv3_x.2.batch_norm2.num_batches_tracked', 'conv3_x.2.conv3.weight', 'conv3_x.2.conv3.bias', 'conv3_x.2.batch_norm3.weight', 'conv3_x.2.batch_norm3.bias', 'conv3_x.2.batch_norm3.running_mean', 'conv3_x.2.batch_norm3.running_var', 'conv3_x.2.batch_norm3.num_batches_tracked', 'conv3_x.3.conv1.weight', 'conv3_x.3.conv1.bias', 'conv3_x.3.batch_norm1.weight', 'conv3_x.3.batch_norm1.bias', 'conv3_x.3.batch_norm1.running_mean', 'conv3_x.3.batch_norm1.running_var', 'conv3_x.3.batch_norm1.num_batches_tracked', 'conv3_x.3.conv2.weight', 'conv3_x.3.conv2.bias', 'conv3_x.3.batch_norm2.weight', 'conv3_x.3.batch_norm2.bias', 'conv3_x.3.batch_norm2.running_mean', 'conv3_x.3.batch_norm2.running_var', 'conv3_x.3.batch_norm2.num_batches_tracked', 'conv3_x.3.conv3.weight', 'conv3_x.3.conv3.bias', 'conv3_x.3.batch_norm3.weight', 'conv3_x.3.batch_norm3.bias', 'conv3_x.3.batch_norm3.running_mean', 'conv3_x.3.batch_norm3.running_var', 'conv3_x.3.batch_norm3.num_batches_tracked', 'shortcut4.0.weight', 'shortcut4.0.bias', 'shortcut4.1.weight', 'shortcut4.1.bias', 'shortcut4.1.running_mean', 'shortcut4.1.running_var', 'shortcut4.1.num_batches_tracked', 'conv4_x.0.conv1.weight', 'conv4_x.0.conv1.bias', 'conv4_x.0.batch_norm1.weight', 'conv4_x.0.batch_norm1.bias', 'conv4_x.0.batch_norm1.running_mean', 'conv4_x.0.batch_norm1.running_var', 'conv4_x.0.batch_norm1.num_batches_tracked', 'conv4_x.0.conv2.weight', 'conv4_x.0.conv2.bias', 'conv4_x.0.batch_norm2.weight', 'conv4_x.0.batch_norm2.bias', 'conv4_x.0.batch_norm2.running_mean', 'conv4_x.0.batch_norm2.running_var', 'conv4_x.0.batch_norm2.num_batches_tracked', 'conv4_x.0.conv3.weight', 'conv4_x.0.conv3.bias', 'conv4_x.0.batch_norm3.weight', 'conv4_x.0.batch_norm3.bias', 'conv4_x.0.batch_norm3.running_mean', 'conv4_x.0.batch_norm3.running_var', 'conv4_x.0.batch_norm3.num_batches_tracked', 'conv4_x.0.shortcut.0.weight', 'conv4_x.0.shortcut.0.bias', 'conv4_x.0.shortcut.1.weight', 'conv4_x.0.shortcut.1.bias', 'conv4_x.0.shortcut.1.running_mean', 'conv4_x.0.shortcut.1.running_var', 'conv4_x.0.shortcut.1.num_batches_tracked', 'conv4_x.1.conv1.weight', 'conv4_x.1.conv1.bias', 'conv4_x.1.batch_norm1.weight', 'conv4_x.1.batch_norm1.bias', 'conv4_x.1.batch_norm1.running_mean', 'conv4_x.1.batch_norm1.running_var', 'conv4_x.1.batch_norm1.num_batches_tracked', 'conv4_x.1.conv2.weight', 'conv4_x.1.conv2.bias', 'conv4_x.1.batch_norm2.weight', 'conv4_x.1.batch_norm2.bias', 'conv4_x.1.batch_norm2.running_mean', 'conv4_x.1.batch_norm2.running_var', 'conv4_x.1.batch_norm2.num_batches_tracked', 'conv4_x.1.conv3.weight', 'conv4_x.1.conv3.bias', 'conv4_x.1.batch_norm3.weight', 'conv4_x.1.batch_norm3.bias', 'conv4_x.1.batch_norm3.running_mean', 'conv4_x.1.batch_norm3.running_var', 'conv4_x.1.batch_norm3.num_batches_tracked', 'conv4_x.2.conv1.weight', 'conv4_x.2.conv1.bias', 'conv4_x.2.batch_norm1.weight', 'conv4_x.2.batch_norm1.bias', 'conv4_x.2.batch_norm1.running_mean', 'conv4_x.2.batch_norm1.running_var', 'conv4_x.2.batch_norm1.num_batches_tracked', 'conv4_x.2.conv2.weight', 'conv4_x.2.conv2.bias', 'conv4_x.2.batch_norm2.weight', 'conv4_x.2.batch_norm2.bias', 'conv4_x.2.batch_norm2.running_mean', 'conv4_x.2.batch_norm2.running_var', 'conv4_x.2.batch_norm2.num_batches_tracked', 'conv4_x.2.conv3.weight', 'conv4_x.2.conv3.bias', 'conv4_x.2.batch_norm3.weight', 'conv4_x.2.batch_norm3.bias', 'conv4_x.2.batch_norm3.running_mean', 'conv4_x.2.batch_norm3.running_var', 'conv4_x.2.batch_norm3.num_batches_tracked', 'conv4_x.3.conv1.weight', 'conv4_x.3.conv1.bias', 'conv4_x.3.batch_norm1.weight', 'conv4_x.3.batch_norm1.bias', 'conv4_x.3.batch_norm1.running_mean', 'conv4_x.3.batch_norm1.running_var', 'conv4_x.3.batch_norm1.num_batches_tracked', 'conv4_x.3.conv2.weight', 'conv4_x.3.conv2.bias', 'conv4_x.3.batch_norm2.weight', 'conv4_x.3.batch_norm2.bias', 'conv4_x.3.batch_norm2.running_mean', 'conv4_x.3.batch_norm2.running_var', 'conv4_x.3.batch_norm2.num_batches_tracked', 'conv4_x.3.conv3.weight', 'conv4_x.3.conv3.bias', 'conv4_x.3.batch_norm3.weight', 'conv4_x.3.batch_norm3.bias', 'conv4_x.3.batch_norm3.running_mean', 'conv4_x.3.batch_norm3.running_var', 'conv4_x.3.batch_norm3.num_batches_tracked', 'conv4_x.4.conv1.weight', 'conv4_x.4.conv1.bias', 'conv4_x.4.batch_norm1.weight', 'conv4_x.4.batch_norm1.bias', 'conv4_x.4.batch_norm1.running_mean', 'conv4_x.4.batch_norm1.running_var', 'conv4_x.4.batch_norm1.num_batches_tracked', 'conv4_x.4.conv2.weight', 'conv4_x.4.conv2.bias', 'conv4_x.4.batch_norm2.weight', 'conv4_x.4.batch_norm2.bias', 'conv4_x.4.batch_norm2.running_mean', 'conv4_x.4.batch_norm2.running_var', 'conv4_x.4.batch_norm2.num_batches_tracked', 'conv4_x.4.conv3.weight', 'conv4_x.4.conv3.bias', 'conv4_x.4.batch_norm3.weight', 'conv4_x.4.batch_norm3.bias', 'conv4_x.4.batch_norm3.running_mean', 'conv4_x.4.batch_norm3.running_var', 'conv4_x.4.batch_norm3.num_batches_tracked', 'conv4_x.5.conv1.weight', 'conv4_x.5.conv1.bias', 'conv4_x.5.batch_norm1.weight', 'conv4_x.5.batch_norm1.bias', 'conv4_x.5.batch_norm1.running_mean', 'conv4_x.5.batch_norm1.running_var', 'conv4_x.5.batch_norm1.num_batches_tracked', 'conv4_x.5.conv2.weight', 'conv4_x.5.conv2.bias', 'conv4_x.5.batch_norm2.weight', 'conv4_x.5.batch_norm2.bias', 'conv4_x.5.batch_norm2.running_mean', 'conv4_x.5.batch_norm2.running_var', 'conv4_x.5.batch_norm2.num_batches_tracked', 'conv4_x.5.conv3.weight', 'conv4_x.5.conv3.bias', 'conv4_x.5.batch_norm3.weight', 'conv4_x.5.batch_norm3.bias', 'conv4_x.5.batch_norm3.running_mean', 'conv4_x.5.batch_norm3.running_var', 'conv4_x.5.batch_norm3.num_batches_tracked', 'shortcut5.0.weight', 'shortcut5.0.bias', 'shortcut5.1.weight', 'shortcut5.1.bias', 'shortcut5.1.running_mean', 'shortcut5.1.running_var', 'shortcut5.1.num_batches_tracked', 'conv5_x.0.conv1.weight', 'conv5_x.0.conv1.bias', 'conv5_x.0.batch_norm1.weight', 'conv5_x.0.batch_norm1.bias', 'conv5_x.0.batch_norm1.running_mean', 'conv5_x.0.batch_norm1.running_var', 'conv5_x.0.batch_norm1.num_batches_tracked', 'conv5_x.0.conv2.weight', 'conv5_x.0.conv2.bias', 'conv5_x.0.batch_norm2.weight', 'conv5_x.0.batch_norm2.bias', 'conv5_x.0.batch_norm2.running_mean', 'conv5_x.0.batch_norm2.running_var', 'conv5_x.0.batch_norm2.num_batches_tracked', 'conv5_x.0.conv3.weight', 'conv5_x.0.conv3.bias', 'conv5_x.0.batch_norm3.weight', 'conv5_x.0.batch_norm3.bias', 'conv5_x.0.batch_norm3.running_mean', 'conv5_x.0.batch_norm3.running_var', 'conv5_x.0.batch_norm3.num_batches_tracked', 'conv5_x.0.shortcut.0.weight', 'conv5_x.0.shortcut.0.bias', 'conv5_x.0.shortcut.1.weight', 'conv5_x.0.shortcut.1.bias', 'conv5_x.0.shortcut.1.running_mean', 'conv5_x.0.shortcut.1.running_var', 'conv5_x.0.shortcut.1.num_batches_tracked', 'conv5_x.1.conv1.weight', 'conv5_x.1.conv1.bias', 'conv5_x.1.batch_norm1.weight', 'conv5_x.1.batch_norm1.bias', 'conv5_x.1.batch_norm1.running_mean', 'conv5_x.1.batch_norm1.running_var', 'conv5_x.1.batch_norm1.num_batches_tracked', 'conv5_x.1.conv2.weight', 'conv5_x.1.conv2.bias', 'conv5_x.1.batch_norm2.weight', 'conv5_x.1.batch_norm2.bias', 'conv5_x.1.batch_norm2.running_mean', 'conv5_x.1.batch_norm2.running_var', 'conv5_x.1.batch_norm2.num_batches_tracked', 'conv5_x.1.conv3.weight', 'conv5_x.1.conv3.bias', 'conv5_x.1.batch_norm3.weight', 'conv5_x.1.batch_norm3.bias', 'conv5_x.1.batch_norm3.running_mean', 'conv5_x.1.batch_norm3.running_var', 'conv5_x.1.batch_norm3.num_batches_tracked', 'conv5_x.2.conv1.weight', 'conv5_x.2.conv1.bias', 'conv5_x.2.batch_norm1.weight', 'conv5_x.2.batch_norm1.bias', 'conv5_x.2.batch_norm1.running_mean', 'conv5_x.2.batch_norm1.running_var', 'conv5_x.2.batch_norm1.num_batches_tracked', 'conv5_x.2.conv2.weight', 'conv5_x.2.conv2.bias', 'conv5_x.2.batch_norm2.weight', 'conv5_x.2.batch_norm2.bias', 'conv5_x.2.batch_norm2.running_mean', 'conv5_x.2.batch_norm2.running_var', 'conv5_x.2.batch_norm2.num_batches_tracked', 'conv5_x.2.conv3.weight', 'conv5_x.2.conv3.bias', 'conv5_x.2.batch_norm3.weight', 'conv5_x.2.batch_norm3.bias', 'conv5_x.2.batch_norm3.running_mean', 'conv5_x.2.batch_norm3.running_var', 'conv5_x.2.batch_norm3.num_batches_tracked', 'classifier.0.weight', 'classifier.0.bias'])"},"metadata":{}}]},{"cell_type":"code","source":"# 읽어들인 모델 파라미터는 모델 아키텍처에 연결을 시켜줘야 함\n# load_state_dict() 사용\nlast_model = ResNet50()\nlast_model.to(device)\nlast_model.load_state_dict(last_state_dict)","metadata":{"execution":{"iopub.status.busy":"2023-04-17T05:48:43.800274Z","iopub.execute_input":"2023-04-17T05:48:43.800863Z","iopub.status.idle":"2023-04-17T05:48:44.178786Z","shell.execute_reply.started":"2023-04-17T05:48:43.800811Z","shell.execute_reply":"2023-04-17T05:48:44.177801Z"},"trusted":true},"execution_count":81,"outputs":[{"execution_count":81,"output_type":"execute_result","data":{"text/plain":"<All keys matched successfully>"},"metadata":{}}]},{"cell_type":"code","source":"last_model.eval()\nevaluation(last_model, testloader, loss_fn) ","metadata":{"execution":{"iopub.status.busy":"2023-04-17T05:49:04.819336Z","iopub.execute_input":"2023-04-17T05:49:04.819688Z","iopub.status.idle":"2023-04-17T05:49:34.718096Z","shell.execute_reply.started":"2023-04-17T05:49:04.819655Z","shell.execute_reply":"2023-04-17T05:49:34.717110Z"},"trusted":true},"execution_count":82,"outputs":[{"name":"stdout","text":"Test Loss : 1.320 Test Accuracy : 0.625\n","output_type":"stream"}]},{"cell_type":"code","source":"# valid loss or accuracy 기준 best model\nbest_state_dict = torch.load('best_checkpoint.pth')\nbest_model = ResNet50()\nbest_model.to(device)\nbest_model.load_state_dict(best_state_dict)","metadata":{"execution":{"iopub.status.busy":"2023-04-17T05:53:04.199988Z","iopub.execute_input":"2023-04-17T05:53:04.200352Z","iopub.status.idle":"2023-04-17T05:53:04.540009Z","shell.execute_reply.started":"2023-04-17T05:53:04.200320Z","shell.execute_reply":"2023-04-17T05:53:04.538855Z"},"trusted":true},"execution_count":85,"outputs":[{"execution_count":85,"output_type":"execute_result","data":{"text/plain":"<All keys matched successfully>"},"metadata":{}}]},{"cell_type":"code","source":"best_model.eval()\nevaluation(best_model, testloader, loss_fn)","metadata":{"execution":{"iopub.status.busy":"2023-04-17T05:53:11.038203Z","iopub.execute_input":"2023-04-17T05:53:11.038913Z","iopub.status.idle":"2023-04-17T05:53:40.715223Z","shell.execute_reply.started":"2023-04-17T05:53:11.038874Z","shell.execute_reply":"2023-04-17T05:53:40.714096Z"},"trusted":true},"execution_count":86,"outputs":[{"name":"stdout","text":"Test Loss : 1.498 Test Accuracy : 0.688\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}